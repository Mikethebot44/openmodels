{"version":3,"sources":["../../src/index.ts","../../src/providers/modal.ts","../../src/providers/base.ts","../../src/streaming.ts","../../src/registry.ts","../../src/config/service-urls.ts","../../src/client.ts","../../src/model-capabilities.ts"],"sourcesContent":["export { OpenModels, client } from './client';\r\nexport { parseSSEStream, OpenModelsError } from './streaming';\r\nexport { ModalProvider } from './providers/modal';\r\nexport { getModelCapabilities, type ModelCapabilities } from './model-capabilities';\r\nexport * from './types';\r\n\r\n","import fetch from 'node-fetch';\nimport { BaseProvider } from './base';\nimport { ChatCompletionRequest, EmbeddingRequest, ImageRequest, AudioTranscribeRequest, AudioSummarizeRequest, ImageClassificationRequest } from '../types';\nimport { OpenModelsError } from '../streaming';\n\nexport class ModalProvider extends BaseProvider {\n  async chat(request: ChatCompletionRequest): Promise<any> {\n    const url = `${this.config.baseUrl}/chat`;\n    \n    const headers: Record<string, string> = {\n      'Content-Type': 'application/json',\n      'Authorization': `Bearer ${this.config.apiKey}`,\n    };\n\n    const response = await fetch(url, {\n      method: 'POST',\n      headers,\n      body: JSON.stringify(request),\n    });\n\n    if (!response.ok) {\n      const errorText = await response.text();\n      \n      if (response.status === 401) {\n        throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n      }\n      if (response.status === 403) {\n        throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n      }\n      \n      throw new OpenModelsError(`Modal API error: ${response.status} ${errorText}`, response.status);\n    }\n\n    return response;\n  }\n\n  async embed(request: EmbeddingRequest): Promise<any> {\n    const url = `${this.config.baseUrl}/embed`;\n    \n    const headers: Record<string, string> = {\n      'Content-Type': 'application/json',\n      'Authorization': `Bearer ${this.config.apiKey}`,\n    };\n\n    const response = await fetch(url, {\n      method: 'POST',\n      headers,\n      body: JSON.stringify(request),\n    });\n\n    if (!response.ok) {\n      const errorText = await response.text();\n      \n      if (response.status === 401) {\n        throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n      }\n      if (response.status === 403) {\n        throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n      }\n      \n      throw new OpenModelsError(`Modal API error: ${response.status} ${errorText}`, response.status);\n    }\n\n    return response;\n  }\n\n  async image(request: ImageRequest): Promise<any> {\n    const url = `${this.config.baseUrl}/image`;\n    \n    const headers: Record<string, string> = {\n      'Content-Type': 'application/json',\n      'Authorization': `Bearer ${this.config.apiKey}`,\n    };\n\n    const response = await fetch(url, {\n      method: 'POST',\n      headers,\n      body: JSON.stringify(request),\n    });\n\n    if (!response.ok) {\n      const errorText = await response.text();\n      \n      if (response.status === 401) {\n        throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n      }\n      if (response.status === 403) {\n        throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n      }\n      \n      throw new OpenModelsError(`Modal API error: ${response.status} ${errorText}`, response.status);\n    }\n\n    return response;\n  }\n\n  async audioTranscribe(request: AudioTranscribeRequest): Promise<any> {\n    const url = `${this.config.baseUrl}/transcribe`;\n    const headers: Record<string, string> = {\n      'Content-Type': 'application/json',\n      'Authorization': `Bearer ${this.config.apiKey}`,\n    };\n    const response = await fetch(url, { method: 'POST', headers, body: JSON.stringify(request) });\n    if (!response.ok) {\n      const errorText = await response.text();\n      if (response.status === 401) throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n      if (response.status === 403) throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n      throw new OpenModelsError(`Modal API error: ${response.status} ${errorText}`, response.status);\n    }\n    return response;\n  }\n\n  async audioSummarize(request: AudioSummarizeRequest): Promise<any> {\n    const url = `${this.config.baseUrl}/summarize`;\n    const headers: Record<string, string> = {\n      'Content-Type': 'application/json',\n      'Authorization': `Bearer ${this.config.apiKey}`,\n    };\n    const response = await fetch(url, { method: 'POST', headers, body: JSON.stringify(request) });\n    if (!response.ok) {\n      const errorText = await response.text();\n      if (response.status === 401) throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n      if (response.status === 403) throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n      throw new OpenModelsError(`Modal API error: ${response.status} ${errorText}`, response.status);\n    }\n    return response;\n  }\n\n  async visionClassify(request: ImageClassificationRequest): Promise<any> {\n    const url = `${this.config.baseUrl}/classify`;\n    const headers: Record<string, string> = {\n      'Content-Type': 'application/json',\n      'Authorization': `Bearer ${this.config.apiKey}`,\n    };\n    const response = await fetch(url, { method: 'POST', headers, body: JSON.stringify(request) });\n    if (!response.ok) {\n      const errorText = await response.text();\n      if (response.status === 401) throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n      if (response.status === 403) throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n      throw new OpenModelsError(`Modal API error: ${response.status} ${errorText}`, response.status);\n    }\n    return response;\n  }\n}\n","import { ProviderConfig } from '../types';\nimport { AudioSummarizeRequest, AudioTranscribeRequest, ImageClassificationRequest } from '../types';\n\nexport interface Provider {\n  chat(request: any): Promise<any>;\n  embed(request: any): Promise<any>;\n  image(request: any): Promise<any>;\n  audioTranscribe?(request: AudioTranscribeRequest): Promise<any>;\n  audioSummarize?(request: AudioSummarizeRequest): Promise<any>;\n  visionClassify?(request: ImageClassificationRequest): Promise<any>;\n}\n\nexport abstract class BaseProvider implements Provider {\n  constructor(protected config: ProviderConfig) {}\n  \n  abstract chat(request: any): Promise<any>;\n  abstract embed(request: any): Promise<any>;\n  abstract image(request: any): Promise<any>;\n  audioTranscribe?(request: AudioTranscribeRequest): Promise<any>;\n  audioSummarize?(request: AudioSummarizeRequest): Promise<any>;\n  visionClassify?(request: ImageClassificationRequest): Promise<any>;\n}\n","import { StreamChunk } from './types';\n\nexport class OpenModelsError extends Error {\n  constructor(\n    message: string,\n    public status?: number,\n    public code?: string\n  ) {\n    super(message);\n    this.name = 'OpenModelsError';\n  }\n}\n\nexport async function* parseSSEStream(\n  response: any\n): AsyncGenerator<string, void, unknown> {\n  if (!response.body) {\n    throw new OpenModelsError('Response body is null');\n  }\n\n  // Handle authentication errors before parsing stream\n  if (response.status === 401) {\n    throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n  }\n  if (response.status === 403) {\n    throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n  }\n\n  // Handle node-fetch response\n  const text = await response.text();\n  const lines = text.split('\\n');\n\n  for (const line of lines) {\n    const trimmed = line.trim();\n    \n    if (trimmed === '') continue;\n    if (trimmed === '[DONE]') return;\n    \n    if (trimmed.startsWith('data: ')) {\n      const data = trimmed.slice(6);\n      if (data === '[DONE]') return;\n      \n      try {\n        const parsed: StreamChunk = JSON.parse(data);\n        if (parsed.choices?.[0]?.delta?.content) {\n          yield parsed.choices[0].delta.content;\n        }\n      } catch (e) {\n        // Skip malformed JSON\n        continue;\n      }\n    }\n  }\n}\n","import { Task } from './types/run';\r\n\r\nexport const TASK_TO_MODELS: Record<Task, string[]> = {\r\n  'image-classification': [\r\n    'google/vit-base-patch16-224',\r\n    'facebook/convnext-base-224',\r\n    'openai/clip-vit-base-patch32',\r\n  ],\r\n  'text-generation': [\r\n    'microsoft/DialoGPT-medium',\r\n    'meta-llama/Llama-3.1-8B-Instruct',\r\n    'meta-llama/Meta-Llama-3-8B-Instruct',\r\n  ],\r\n  'embedding': [\r\n    'sentence-transformers/all-MiniLM-L6-v2',\r\n  ],\r\n  'audio-transcribe': [\r\n    'openai/whisper-base',\r\n  ],\r\n  'audio-summarize': [\r\n    'facebook/bart-large-cnn',\r\n  ],\r\n  'image-generation': [\r\n    'runwayml/stable-diffusion-v1-5',\r\n  ],\r\n};\r\n\r\nexport function getDefaultModel(task: Task): string {\r\n  const models = TASK_TO_MODELS[task];\r\n  if (!models || models.length === 0) {\r\n    throw new Error(`No default models configured for task: ${task}`);\r\n  }\r\n  return models[0];\r\n}\r\n\r\n\r\n","/**\r\n * Internal configuration for service URLs\r\n * These URLs should not be exposed to end users\r\n */\r\n\r\ninterface ServiceUrls {\r\n  text: string;\r\n  embed: string;\r\n  image: string;\r\n  audio: string;\r\n  vision: string;\r\n}\r\n\r\n/**\r\n * Production Modal service URLs\r\n * @internal\r\n */\r\nexport const MODAL_SERVICE_URLS: ServiceUrls = {\r\n  text: 'https://mikethebot44--tryscout-text-create-app.modal.run',\r\n  embed: 'https://mikethebot44--tryscout-embed-create-app.modal.run',\r\n  image: 'https://mikethebot44--tryscout-image-create-app.modal.run',\r\n  audio: 'https://mikethebot44--tryscout-audio-create-app.modal.run',\r\n  vision: 'https://mikethebot44--tryscout-vision-create-app.modal.run',\r\n};\r\n\r\n/**\r\n * Get the service URL for a specific operation type\r\n * @internal\r\n */\r\nexport function getServiceUrl(service: keyof ServiceUrls): string {\r\n  return MODAL_SERVICE_URLS[service];\r\n}\r\n\r\n","import { ModalProvider } from './providers/modal';\nimport { parseSSEStream, OpenModelsError } from './streaming';\nimport { \n  OpenModelsConfig, \n  ChatCompletionRequest, \n  ChatCompletionResponse,\n  EmbeddingRequest,\n  EmbeddingResponse,\n  ImageRequest,\n  ImageResponse,\n  AudioTranscribeRequest,\n  AudioTranscribeResponse,\n  AudioSummarizeRequest,\n  AudioSummarizeResponse,\n  ImageClassificationRequest,\n  ImageClassificationResponse,\n  RunRequest,\n  RunResponse\n} from './types';\nimport { getDefaultModel } from './registry';\nimport { getServiceUrl } from './config/service-urls';\n\nexport class OpenModels {\n  private textProvider: ModalProvider;\n  private embedProvider: ModalProvider;\n  private imageProvider: ModalProvider;\n  private audioProvider: ModalProvider;\n  private visionProvider: ModalProvider;\n\n  constructor(config: OpenModelsConfig = {}) {\n    // Validate that API key is provided\n    if (!config.apiKey) {\n      throw new OpenModelsError('API key is required. Please provide an API key in the client configuration.');\n    }\n\n    // Validate API key format\n    if (!config.apiKey.startsWith('om_') || config.apiKey.length < 10) {\n      throw new OpenModelsError('Invalid API key format. API keys must start with \"om_\" and be at least 10 characters long.');\n    }\n\n    // Initialize providers with service-specific URLs\n    this.textProvider = new ModalProvider({\n      apiKey: config.apiKey,\n      baseUrl: getServiceUrl('text'),\n    });\n\n    this.embedProvider = new ModalProvider({\n      apiKey: config.apiKey,\n      baseUrl: getServiceUrl('embed'),\n    });\n\n    this.imageProvider = new ModalProvider({\n      apiKey: config.apiKey,\n      baseUrl: getServiceUrl('image'),\n    });\n\n    this.audioProvider = new ModalProvider({\n      apiKey: config.apiKey,\n      baseUrl: getServiceUrl('audio'),\n    });\n\n    this.visionProvider = new ModalProvider({\n      apiKey: config.apiKey,\n      baseUrl: getServiceUrl('vision'),\n    });\n  }\n\n  async chat(request: ChatCompletionRequest): Promise<ChatCompletionResponse | AsyncGenerator<string, void, unknown>> {\n    try {\n      const response = await this.textProvider.chat(request);\n      \n      if (request.stream) {\n        return parseSSEStream(response);\n      }\n      \n      const data = await response.json();\n      return data as ChatCompletionResponse;\n    } catch (error) {\n      if (error instanceof Error) {\n        throw new OpenModelsError(error.message);\n      }\n      throw new OpenModelsError('Unknown error occurred');\n    }\n  }\n\n  async embed(request: EmbeddingRequest): Promise<EmbeddingResponse> {\n    try {\n      const response = await this.embedProvider.embed(request);\n      const data = await response.json();\n      return data as EmbeddingResponse;\n    } catch (error) {\n      if (error instanceof Error) {\n        throw new OpenModelsError(error.message);\n      }\n      throw new OpenModelsError('Unknown error occurred');\n    }\n  }\n\n  async image(request: ImageRequest): Promise<ImageResponse> {\n    try {\n      const response = await this.imageProvider.image(request);\n      const data = await response.json();\n      return data as ImageResponse;\n    } catch (error) {\n      if (error instanceof Error) {\n        throw new OpenModelsError(error.message);\n      }\n      throw new OpenModelsError('Unknown error occurred');\n    }\n  }\n\n  async run(request: RunRequest): Promise<RunResponse | AsyncGenerator<string, void, unknown>> {\n    try {\n      switch (request.task) {\n        case 'text-generation': {\n          const model = (request as any).model || getDefaultModel('text-generation');\n          const chatReq: ChatCompletionRequest = { ...request, model } as any;\n          const response = await this.textProvider.chat(chatReq);\n          if (chatReq.stream) return parseSSEStream(response);\n          return (await response.json()) as ChatCompletionResponse;\n        }\n        case 'image-generation': {\n          const model = (request as any).model || getDefaultModel('image-generation');\n          const imgReq: ImageRequest = { ...request, model } as any;\n          const response = await this.imageProvider.image(imgReq);\n          return (await response.json()) as ImageResponse;\n        }\n        case 'embedding': {\n          const model = (request as any).model || getDefaultModel('embedding');\n          const embReq: EmbeddingRequest = { ...request, model } as any;\n          const response = await this.embedProvider.embed(embReq);\n          return (await response.json()) as any;\n        }\n        case 'audio-transcribe': {\n          const model = (request as any).model || getDefaultModel('audio-transcribe');\n          const aReq: AudioTranscribeRequest = { ...request, model } as any;\n          const response = await this.audioProvider.audioTranscribe!(aReq);\n          return (await response.json()) as AudioTranscribeResponse;\n        }\n        case 'audio-summarize': {\n          const model = (request as any).model || getDefaultModel('audio-summarize');\n          const aReq: AudioSummarizeRequest = { ...request, model } as any;\n          const response = await this.audioProvider.audioSummarize!(aReq);\n          return (await response.json()) as AudioSummarizeResponse;\n        }\n        case 'image-classification': {\n          const model = (request as any).model || getDefaultModel('image-classification');\n          const vReq: ImageClassificationRequest = { ...request, model } as any;\n          const response = await this.visionProvider.visionClassify!(vReq);\n          return (await response.json()) as ImageClassificationResponse;\n        }\n        default:\n          throw new OpenModelsError('Unsupported task');\n      }\n    } catch (error) {\n      if (error instanceof Error) throw new OpenModelsError(error.message);\n      throw new OpenModelsError('Unknown error occurred');\n    }\n  }\n}\n\nexport function client(config?: OpenModelsConfig): OpenModels {\n  return new OpenModels(config);\n}\n\n","export interface ModelCapabilities {\r\n  supportsTopP: boolean;\r\n  supportsStop: boolean;\r\n  supportsTemperature: boolean;\r\n  supportsMaxTokens: boolean;\r\n  supportsStreaming: boolean;\r\n  supportsQuantization: boolean;\r\n  supportedQuantizations?: ('int8' | 'int4' | 'float16')[];\r\n  maxTokenLimit?: number;\r\n  notes?: string;\r\n}\r\n\r\nexport const TEXT_MODEL_CAPABILITIES: Record<string, ModelCapabilities> = {\r\n  \"microsoft/DialoGPT-medium\": {\r\n    supportsTopP: true,\r\n    supportsStop: true,\r\n    supportsTemperature: true,\r\n    supportsMaxTokens: true,\r\n    supportsStreaming: true,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['int8', 'float16'],\r\n    maxTokenLimit: 1000,\r\n    notes: \"Conversational model, best for dialog\"\r\n  },\r\n  \"microsoft/DialoGPT-large\": {\r\n    supportsTopP: true,\r\n    supportsStop: true,\r\n    supportsTemperature: true,\r\n    supportsMaxTokens: true,\r\n    supportsStreaming: true,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['int8', 'float16'],\r\n    maxTokenLimit: 1000\r\n  },\r\n  \"meta-llama/Llama-3.1-8B-Instruct\": {\r\n    supportsTopP: true,\r\n    supportsStop: true,\r\n    supportsTemperature: true,\r\n    supportsMaxTokens: true,\r\n    supportsStreaming: true,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['int8', 'int4', 'float16'],\r\n    maxTokenLimit: 4096,\r\n    notes: \"Instruction-following model\"\r\n  },\r\n  \"meta-llama/Meta-Llama-3-8B-Instruct\": {\r\n    supportsTopP: true,\r\n    supportsStop: true,\r\n    supportsTemperature: true,\r\n    supportsMaxTokens: true,\r\n    supportsStreaming: true,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['int8', 'int4', 'float16'],\r\n    maxTokenLimit: 4096,\r\n    notes: \"Instruction-following model with chat formatting\"\r\n  },\r\n  \"facebook/blenderbot-400M-distill\": {\r\n    supportsTopP: true,\r\n    supportsStop: false,\r\n    supportsTemperature: true,\r\n    supportsMaxTokens: true,\r\n    supportsStreaming: true,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['int8', 'float16'],\r\n    maxTokenLimit: 128\r\n  },\r\n  \"EleutherAI/gpt-neo-2.7B\": {\r\n    supportsTopP: true,\r\n    supportsStop: true,\r\n    supportsTemperature: true,\r\n    supportsMaxTokens: true,\r\n    supportsStreaming: true,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['int8', 'int4', 'float16'],\r\n    maxTokenLimit: 2048\r\n  }\r\n};\r\n\r\nexport const IMAGE_MODEL_CAPABILITIES: Record<string, ModelCapabilities> = {\r\n  \"runwayml/stable-diffusion-v1-5\": {\r\n    supportsTopP: false,\r\n    supportsStop: false,\r\n    supportsTemperature: false,\r\n    supportsMaxTokens: false,\r\n    supportsStreaming: false,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['float16'],\r\n    notes: \"Size, quality, style parameters supported\"\r\n  },\r\n  \"stabilityai/stable-diffusion-xl-base-1.0\": {\r\n    supportsTopP: false,\r\n    supportsStop: false,\r\n    supportsTemperature: false,\r\n    supportsMaxTokens: false,\r\n    supportsStreaming: false,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['float16'],\r\n    notes: \"Higher quality, slower generation\"\r\n  }\r\n};\r\n\r\nexport const EMBED_MODEL_CAPABILITIES: Record<string, ModelCapabilities> = {\r\n  \"sentence-transformers/all-MiniLM-L6-v2\": {\r\n    supportsTopP: false,\r\n    supportsStop: false,\r\n    supportsTemperature: false,\r\n    supportsMaxTokens: false,\r\n    supportsStreaming: false,\r\n    supportsQuantization: false,\r\n    notes: \"Fast and efficient, good for most tasks\"\r\n  }\r\n};\r\n\r\nexport const VISION_MODEL_CAPABILITIES: Record<string, ModelCapabilities> = {\r\n  \"google/vit-base-patch16-224\": {\r\n    supportsTopP: false,\r\n    supportsStop: false,\r\n    supportsTemperature: false,\r\n    supportsMaxTokens: false,\r\n    supportsStreaming: false,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['int8', 'float16'],\r\n    notes: \"Vision Transformer, good balance\"\r\n  },\r\n  \"facebook/convnext-base-224\": {\r\n    supportsTopP: false,\r\n    supportsStop: false,\r\n    supportsTemperature: false,\r\n    supportsMaxTokens: false,\r\n    supportsStreaming: false,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['int8', 'float16']\r\n  },\r\n  \"openai/clip-vit-base-patch32\": {\r\n    supportsTopP: false,\r\n    supportsStop: false,\r\n    supportsTemperature: false,\r\n    supportsMaxTokens: false,\r\n    supportsStreaming: false,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['int8', 'float16']\r\n  }\r\n};\r\n\r\nexport const AUDIO_MODEL_CAPABILITIES: Record<string, ModelCapabilities> = {\r\n  \"openai/whisper-base\": {\r\n    supportsTopP: false,\r\n    supportsStop: false,\r\n    supportsTemperature: false,\r\n    supportsMaxTokens: false,\r\n    supportsStreaming: false,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['int8', 'float16'],\r\n    notes: \"Fast, good for most languages\"\r\n  },\r\n  \"openai/whisper-small\": {\r\n    supportsTopP: false,\r\n    supportsStop: false,\r\n    supportsTemperature: false,\r\n    supportsMaxTokens: false,\r\n    supportsStreaming: false,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['int8', 'float16'],\r\n    notes: \"Better accuracy than base\"\r\n  },\r\n  \"openai/whisper-medium\": {\r\n    supportsTopP: false,\r\n    supportsStop: false,\r\n    supportsTemperature: false,\r\n    supportsMaxTokens: false,\r\n    supportsStreaming: false,\r\n    supportsQuantization: true,\r\n    supportedQuantizations: ['int8', 'float16'],\r\n    notes: \"High accuracy, slower\"\r\n  }\r\n};\r\n\r\nexport function getModelCapabilities(model: string, type: 'text' | 'image' | 'embed' | 'vision' | 'audio'): ModelCapabilities {\r\n  const defaultCapabilities: ModelCapabilities = {\r\n    supportsTopP: false,\r\n    supportsStop: false,\r\n    supportsTemperature: false,\r\n    supportsMaxTokens: false,\r\n    supportsStreaming: false,\r\n    supportsQuantization: false\r\n  };\r\n\r\n  let registry: Record<string, ModelCapabilities>;\r\n  switch (type) {\r\n    case 'text': registry = TEXT_MODEL_CAPABILITIES; break;\r\n    case 'image': registry = IMAGE_MODEL_CAPABILITIES; break;\r\n    case 'embed': registry = EMBED_MODEL_CAPABILITIES; break;\r\n    case 'vision': registry = VISION_MODEL_CAPABILITIES; break;\r\n    case 'audio': registry = AUDIO_MODEL_CAPABILITIES; break;\r\n    default: return defaultCapabilities;\r\n  }\r\n\r\n  return registry[model] || defaultCapabilities;\r\n}\r\n\r\n"],"mappings":"0jBAAA,IAAAA,EAAA,GAAAC,EAAAD,EAAA,mBAAAE,EAAA,eAAAC,EAAA,oBAAAC,EAAA,WAAAC,EAAA,yBAAAC,EAAA,mBAAAC,IAAA,eAAAC,EAAAR,GCAA,IAAAS,EAAkB,yBCYX,IAAeC,EAAf,KAAgD,CACrD,YAAsBC,EAAwB,CAAxB,YAAAA,CAAyB,CAQjD,ECnBO,IAAMC,EAAN,cAA8B,KAAM,CACzC,YACEC,EACOC,EACAC,EACP,CACA,MAAMF,CAAO,EAHN,YAAAC,EACA,UAAAC,EAGP,KAAK,KAAO,iBACd,CACF,EAEA,eAAuBC,EACrBC,EACuC,CACvC,GAAI,CAACA,EAAS,KACZ,MAAM,IAAIL,EAAgB,uBAAuB,EAInD,GAAIK,EAAS,SAAW,IACtB,MAAM,IAAIL,EAAgB,kDAAmD,GAAG,EAElF,GAAIK,EAAS,SAAW,IACtB,MAAM,IAAIL,EAAgB,oDAAqD,GAAG,EAKpF,IAAMM,GADO,MAAMD,EAAS,KAAK,GACd,MAAM;AAAA,CAAI,EAE7B,QAAWE,KAAQD,EAAO,CACxB,IAAME,EAAUD,EAAK,KAAK,EAE1B,GAAIC,IAAY,GAChB,IAAIA,IAAY,SAAU,OAE1B,GAAIA,EAAQ,WAAW,QAAQ,EAAG,CAChC,IAAMC,EAAOD,EAAQ,MAAM,CAAC,EAC5B,GAAIC,IAAS,SAAU,OAEvB,GAAI,CACF,IAAMC,EAAsB,KAAK,MAAMD,CAAI,EACvCC,EAAO,UAAU,CAAC,GAAG,OAAO,UAC9B,MAAMA,EAAO,QAAQ,CAAC,EAAE,MAAM,QAElC,MAAY,CAEV,QACF,CACF,EACF,CACF,CFhDO,IAAMC,EAAN,cAA4BC,CAAa,CAC9C,MAAM,KAAKC,EAA8C,CACvD,IAAMC,EAAM,GAAG,KAAK,OAAO,OAAO,QAE5BC,EAAkC,CACtC,eAAgB,mBAChB,cAAiB,UAAU,KAAK,OAAO,MAAM,EAC/C,EAEMC,EAAW,QAAM,EAAAC,SAAMH,EAAK,CAChC,OAAQ,OACR,QAAAC,EACA,KAAM,KAAK,UAAUF,CAAO,CAC9B,CAAC,EAED,GAAI,CAACG,EAAS,GAAI,CAChB,IAAME,EAAY,MAAMF,EAAS,KAAK,EAEtC,MAAIA,EAAS,SAAW,IAChB,IAAIG,EAAgB,kDAAmD,GAAG,EAE9EH,EAAS,SAAW,IAChB,IAAIG,EAAgB,oDAAqD,GAAG,EAG9E,IAAIA,EAAgB,oBAAoBH,EAAS,MAAM,IAAIE,CAAS,GAAIF,EAAS,MAAM,CAC/F,CAEA,OAAOA,CACT,CAEA,MAAM,MAAMH,EAAyC,CACnD,IAAMC,EAAM,GAAG,KAAK,OAAO,OAAO,SAE5BC,EAAkC,CACtC,eAAgB,mBAChB,cAAiB,UAAU,KAAK,OAAO,MAAM,EAC/C,EAEMC,EAAW,QAAM,EAAAC,SAAMH,EAAK,CAChC,OAAQ,OACR,QAAAC,EACA,KAAM,KAAK,UAAUF,CAAO,CAC9B,CAAC,EAED,GAAI,CAACG,EAAS,GAAI,CAChB,IAAME,EAAY,MAAMF,EAAS,KAAK,EAEtC,MAAIA,EAAS,SAAW,IAChB,IAAIG,EAAgB,kDAAmD,GAAG,EAE9EH,EAAS,SAAW,IAChB,IAAIG,EAAgB,oDAAqD,GAAG,EAG9E,IAAIA,EAAgB,oBAAoBH,EAAS,MAAM,IAAIE,CAAS,GAAIF,EAAS,MAAM,CAC/F,CAEA,OAAOA,CACT,CAEA,MAAM,MAAMH,EAAqC,CAC/C,IAAMC,EAAM,GAAG,KAAK,OAAO,OAAO,SAE5BC,EAAkC,CACtC,eAAgB,mBAChB,cAAiB,UAAU,KAAK,OAAO,MAAM,EAC/C,EAEMC,EAAW,QAAM,EAAAC,SAAMH,EAAK,CAChC,OAAQ,OACR,QAAAC,EACA,KAAM,KAAK,UAAUF,CAAO,CAC9B,CAAC,EAED,GAAI,CAACG,EAAS,GAAI,CAChB,IAAME,EAAY,MAAMF,EAAS,KAAK,EAEtC,MAAIA,EAAS,SAAW,IAChB,IAAIG,EAAgB,kDAAmD,GAAG,EAE9EH,EAAS,SAAW,IAChB,IAAIG,EAAgB,oDAAqD,GAAG,EAG9E,IAAIA,EAAgB,oBAAoBH,EAAS,MAAM,IAAIE,CAAS,GAAIF,EAAS,MAAM,CAC/F,CAEA,OAAOA,CACT,CAEA,MAAM,gBAAgBH,EAA+C,CACnE,IAAMC,EAAM,GAAG,KAAK,OAAO,OAAO,cAC5BC,EAAkC,CACtC,eAAgB,mBAChB,cAAiB,UAAU,KAAK,OAAO,MAAM,EAC/C,EACMC,EAAW,QAAM,EAAAC,SAAMH,EAAK,CAAE,OAAQ,OAAQ,QAAAC,EAAS,KAAM,KAAK,UAAUF,CAAO,CAAE,CAAC,EAC5F,GAAI,CAACG,EAAS,GAAI,CAChB,IAAME,EAAY,MAAMF,EAAS,KAAK,EACtC,MAAIA,EAAS,SAAW,IAAW,IAAIG,EAAgB,kDAAmD,GAAG,EACzGH,EAAS,SAAW,IAAW,IAAIG,EAAgB,oDAAqD,GAAG,EACzG,IAAIA,EAAgB,oBAAoBH,EAAS,MAAM,IAAIE,CAAS,GAAIF,EAAS,MAAM,CAC/F,CACA,OAAOA,CACT,CAEA,MAAM,eAAeH,EAA8C,CACjE,IAAMC,EAAM,GAAG,KAAK,OAAO,OAAO,aAC5BC,EAAkC,CACtC,eAAgB,mBAChB,cAAiB,UAAU,KAAK,OAAO,MAAM,EAC/C,EACMC,EAAW,QAAM,EAAAC,SAAMH,EAAK,CAAE,OAAQ,OAAQ,QAAAC,EAAS,KAAM,KAAK,UAAUF,CAAO,CAAE,CAAC,EAC5F,GAAI,CAACG,EAAS,GAAI,CAChB,IAAME,EAAY,MAAMF,EAAS,KAAK,EACtC,MAAIA,EAAS,SAAW,IAAW,IAAIG,EAAgB,kDAAmD,GAAG,EACzGH,EAAS,SAAW,IAAW,IAAIG,EAAgB,oDAAqD,GAAG,EACzG,IAAIA,EAAgB,oBAAoBH,EAAS,MAAM,IAAIE,CAAS,GAAIF,EAAS,MAAM,CAC/F,CACA,OAAOA,CACT,CAEA,MAAM,eAAeH,EAAmD,CACtE,IAAMC,EAAM,GAAG,KAAK,OAAO,OAAO,YAC5BC,EAAkC,CACtC,eAAgB,mBAChB,cAAiB,UAAU,KAAK,OAAO,MAAM,EAC/C,EACMC,EAAW,QAAM,EAAAC,SAAMH,EAAK,CAAE,OAAQ,OAAQ,QAAAC,EAAS,KAAM,KAAK,UAAUF,CAAO,CAAE,CAAC,EAC5F,GAAI,CAACG,EAAS,GAAI,CAChB,IAAME,EAAY,MAAMF,EAAS,KAAK,EACtC,MAAIA,EAAS,SAAW,IAAW,IAAIG,EAAgB,kDAAmD,GAAG,EACzGH,EAAS,SAAW,IAAW,IAAIG,EAAgB,oDAAqD,GAAG,EACzG,IAAIA,EAAgB,oBAAoBH,EAAS,MAAM,IAAIE,CAAS,GAAIF,EAAS,MAAM,CAC/F,CACA,OAAOA,CACT,CACF,EG7IO,IAAMI,EAAyC,CACpD,uBAAwB,CACtB,8BACA,6BACA,8BACF,EACA,kBAAmB,CACjB,4BACA,mCACA,qCACF,EACA,UAAa,CACX,wCACF,EACA,mBAAoB,CAClB,qBACF,EACA,kBAAmB,CACjB,yBACF,EACA,mBAAoB,CAClB,gCACF,CACF,EAEO,SAASC,EAAgBC,EAAoB,CAClD,IAAMC,EAASH,EAAeE,CAAI,EAClC,GAAI,CAACC,GAAUA,EAAO,SAAW,EAC/B,MAAM,IAAI,MAAM,0CAA0CD,CAAI,EAAE,EAElE,OAAOC,EAAO,CAAC,CACjB,CChBO,IAAMC,EAAkC,CAC7C,KAAM,2DACN,MAAO,4DACP,MAAO,4DACP,MAAO,4DACP,OAAQ,4DACV,EAMO,SAASC,EAAcC,EAAoC,CAChE,OAAOF,EAAmBE,CAAO,CACnC,CCTO,IAAMC,EAAN,KAAiB,CAOtB,YAAYC,EAA2B,CAAC,EAAG,CAEzC,GAAI,CAACA,EAAO,OACV,MAAM,IAAIC,EAAgB,6EAA6E,EAIzG,GAAI,CAACD,EAAO,OAAO,WAAW,KAAK,GAAKA,EAAO,OAAO,OAAS,GAC7D,MAAM,IAAIC,EAAgB,4FAA4F,EAIxH,KAAK,aAAe,IAAIC,EAAc,CACpC,OAAQF,EAAO,OACf,QAASG,EAAc,MAAM,CAC/B,CAAC,EAED,KAAK,cAAgB,IAAID,EAAc,CACrC,OAAQF,EAAO,OACf,QAASG,EAAc,OAAO,CAChC,CAAC,EAED,KAAK,cAAgB,IAAID,EAAc,CACrC,OAAQF,EAAO,OACf,QAASG,EAAc,OAAO,CAChC,CAAC,EAED,KAAK,cAAgB,IAAID,EAAc,CACrC,OAAQF,EAAO,OACf,QAASG,EAAc,OAAO,CAChC,CAAC,EAED,KAAK,eAAiB,IAAID,EAAc,CACtC,OAAQF,EAAO,OACf,QAASG,EAAc,QAAQ,CACjC,CAAC,CACH,CAEA,MAAM,KAAKC,EAAyG,CAClH,GAAI,CACF,IAAMC,EAAW,MAAM,KAAK,aAAa,KAAKD,CAAO,EAErD,OAAIA,EAAQ,OACHE,EAAeD,CAAQ,EAGnB,MAAMA,EAAS,KAAK,CAEnC,OAASE,EAAO,CACd,MAAIA,aAAiB,MACb,IAAIN,EAAgBM,EAAM,OAAO,EAEnC,IAAIN,EAAgB,wBAAwB,CACpD,CACF,CAEA,MAAM,MAAMG,EAAuD,CACjE,GAAI,CAGF,OADa,MADI,MAAM,KAAK,cAAc,MAAMA,CAAO,GAC3B,KAAK,CAEnC,OAASG,EAAO,CACd,MAAIA,aAAiB,MACb,IAAIN,EAAgBM,EAAM,OAAO,EAEnC,IAAIN,EAAgB,wBAAwB,CACpD,CACF,CAEA,MAAM,MAAMG,EAA+C,CACzD,GAAI,CAGF,OADa,MADI,MAAM,KAAK,cAAc,MAAMA,CAAO,GAC3B,KAAK,CAEnC,OAASG,EAAO,CACd,MAAIA,aAAiB,MACb,IAAIN,EAAgBM,EAAM,OAAO,EAEnC,IAAIN,EAAgB,wBAAwB,CACpD,CACF,CAEA,MAAM,IAAIG,EAAmF,CAC3F,GAAI,CACF,OAAQA,EAAQ,KAAM,CACpB,IAAK,kBAAmB,CACtB,IAAMI,EAASJ,EAAgB,OAASK,EAAgB,iBAAiB,EACnEC,EAAiC,CAAE,GAAGN,EAAS,MAAAI,CAAM,EACrDH,EAAW,MAAM,KAAK,aAAa,KAAKK,CAAO,EACrD,OAAIA,EAAQ,OAAeJ,EAAeD,CAAQ,EAC1C,MAAMA,EAAS,KAAK,CAC9B,CACA,IAAK,mBAAoB,CACvB,IAAMG,EAASJ,EAAgB,OAASK,EAAgB,kBAAkB,EACpEE,EAAuB,CAAE,GAAGP,EAAS,MAAAI,CAAM,EAEjD,OAAQ,MADS,MAAM,KAAK,cAAc,MAAMG,CAAM,GAC/B,KAAK,CAC9B,CACA,IAAK,YAAa,CAChB,IAAMH,EAASJ,EAAgB,OAASK,EAAgB,WAAW,EAC7DG,EAA2B,CAAE,GAAGR,EAAS,MAAAI,CAAM,EAErD,OAAQ,MADS,MAAM,KAAK,cAAc,MAAMI,CAAM,GAC/B,KAAK,CAC9B,CACA,IAAK,mBAAoB,CACvB,IAAMJ,EAASJ,EAAgB,OAASK,EAAgB,kBAAkB,EACpEI,EAA+B,CAAE,GAAGT,EAAS,MAAAI,CAAM,EAEzD,OAAQ,MADS,MAAM,KAAK,cAAc,gBAAiBK,CAAI,GACxC,KAAK,CAC9B,CACA,IAAK,kBAAmB,CACtB,IAAML,EAASJ,EAAgB,OAASK,EAAgB,iBAAiB,EACnEI,EAA8B,CAAE,GAAGT,EAAS,MAAAI,CAAM,EAExD,OAAQ,MADS,MAAM,KAAK,cAAc,eAAgBK,CAAI,GACvC,KAAK,CAC9B,CACA,IAAK,uBAAwB,CAC3B,IAAML,EAASJ,EAAgB,OAASK,EAAgB,sBAAsB,EACxEK,EAAmC,CAAE,GAAGV,EAAS,MAAAI,CAAM,EAE7D,OAAQ,MADS,MAAM,KAAK,eAAe,eAAgBM,CAAI,GACxC,KAAK,CAC9B,CACA,QACE,MAAM,IAAIb,EAAgB,kBAAkB,CAChD,CACF,OAASM,EAAO,CACd,MAAIA,aAAiB,MAAa,IAAIN,EAAgBM,EAAM,OAAO,EAC7D,IAAIN,EAAgB,wBAAwB,CACpD,CACF,CACF,EAEO,SAASc,EAAOf,EAAuC,CAC5D,OAAO,IAAID,EAAWC,CAAM,CAC9B,CCvJO,IAAMgB,EAA6D,CACxE,4BAA6B,CAC3B,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,OAAQ,SAAS,EAC1C,cAAe,IACf,MAAO,uCACT,EACA,2BAA4B,CAC1B,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,OAAQ,SAAS,EAC1C,cAAe,GACjB,EACA,mCAAoC,CAClC,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,OAAQ,OAAQ,SAAS,EAClD,cAAe,KACf,MAAO,6BACT,EACA,sCAAuC,CACrC,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,OAAQ,OAAQ,SAAS,EAClD,cAAe,KACf,MAAO,kDACT,EACA,mCAAoC,CAClC,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,OAAQ,SAAS,EAC1C,cAAe,GACjB,EACA,0BAA2B,CACzB,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,OAAQ,OAAQ,SAAS,EAClD,cAAe,IACjB,CACF,EAEaC,EAA8D,CACzE,iCAAkC,CAChC,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,SAAS,EAClC,MAAO,2CACT,EACA,2CAA4C,CAC1C,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,SAAS,EAClC,MAAO,mCACT,CACF,EAEaC,EAA8D,CACzE,yCAA0C,CACxC,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,MAAO,yCACT,CACF,EAEaC,EAA+D,CAC1E,8BAA+B,CAC7B,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,OAAQ,SAAS,EAC1C,MAAO,kCACT,EACA,6BAA8B,CAC5B,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,OAAQ,SAAS,CAC5C,EACA,+BAAgC,CAC9B,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,OAAQ,SAAS,CAC5C,CACF,EAEaC,EAA8D,CACzE,sBAAuB,CACrB,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,OAAQ,SAAS,EAC1C,MAAO,+BACT,EACA,uBAAwB,CACtB,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,OAAQ,SAAS,EAC1C,MAAO,2BACT,EACA,wBAAyB,CACvB,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,GACtB,uBAAwB,CAAC,OAAQ,SAAS,EAC1C,MAAO,uBACT,CACF,EAEO,SAASC,EAAqBC,EAAeC,EAA0E,CAC5H,IAAMC,EAAyC,CAC7C,aAAc,GACd,aAAc,GACd,oBAAqB,GACrB,kBAAmB,GACnB,kBAAmB,GACnB,qBAAsB,EACxB,EAEIC,EACJ,OAAQF,EAAM,CACZ,IAAK,OAAQE,EAAWT,EAAyB,MACjD,IAAK,QAASS,EAAWR,EAA0B,MACnD,IAAK,QAASQ,EAAWP,EAA0B,MACnD,IAAK,SAAUO,EAAWN,EAA2B,MACrD,IAAK,QAASM,EAAWL,EAA0B,MACnD,QAAS,OAAOI,CAClB,CAEA,OAAOC,EAASH,CAAK,GAAKE,CAC5B","names":["src_exports","__export","ModalProvider","OpenModels","OpenModelsError","client","getModelCapabilities","parseSSEStream","__toCommonJS","import_node_fetch","BaseProvider","config","OpenModelsError","message","status","code","parseSSEStream","response","lines","line","trimmed","data","parsed","ModalProvider","BaseProvider","request","url","headers","response","fetch","errorText","OpenModelsError","TASK_TO_MODELS","getDefaultModel","task","models","MODAL_SERVICE_URLS","getServiceUrl","service","OpenModels","config","OpenModelsError","ModalProvider","getServiceUrl","request","response","parseSSEStream","error","model","getDefaultModel","chatReq","imgReq","embReq","aReq","vReq","client","TEXT_MODEL_CAPABILITIES","IMAGE_MODEL_CAPABILITIES","EMBED_MODEL_CAPABILITIES","VISION_MODEL_CAPABILITIES","AUDIO_MODEL_CAPABILITIES","getModelCapabilities","model","type","defaultCapabilities","registry"]}