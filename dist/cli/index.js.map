{"version":3,"sources":["../../cli/index.ts","../../cli/commands/chat.ts","../../src/providers/huggingface.ts","../../src/streaming.ts","../../src/registry.ts","../../src/client.ts","../../cli/utils.ts","../../cli/commands/embed.ts","../../cli/commands/image.ts","../../cli/commands/models.ts","../../cli/commands/config.ts"],"sourcesContent":["#!/usr/bin/env node\r\n\r\nimport { Command } from 'commander';\r\nimport { chatCommand } from './commands/chat';\r\nimport { embedCommand } from './commands/embed';\r\nimport { imageCommand } from './commands/image';\r\nimport { modelsCommand } from './commands/models';\r\nimport { configCommand } from './commands/config';\r\n\r\nconst program = new Command();\r\n\r\nprogram\r\n  .name('openmodels')\r\n  .description('CLI for OpenModels - Open-source AI models SDK')\r\n  .version('0.3.0');\r\n\r\n// Add commands\r\nprogram.addCommand(chatCommand);\r\nprogram.addCommand(embedCommand);\r\nprogram.addCommand(imageCommand);\r\nprogram.addCommand(modelsCommand);\r\nprogram.addCommand(configCommand);\r\n\r\nprogram.parse();\r\n","import { Command } from 'commander';\nimport { client } from '../../src';\nimport chalk from 'chalk';\nimport ora from 'ora';\nimport { readConfig } from '../utils';\n\nexport const chatCommand = new Command('chat')\n  .description('Chat with AI models')\n  .argument('[message]', 'Message to send to the AI')\n  .option('-m, --model <model>', 'Model to use', 'microsoft/DialoGPT-medium')\n  .option('-s, --stream', 'Stream the response', false)\n  .option('-t, --temperature <temp>', 'Temperature for generation', '0.7')\n  .option('-k, --max-tokens <tokens>', 'Maximum tokens to generate', '200')\n  .option('-i, --interactive', 'Interactive chat mode', false)\n  .action(async (message, options) => {\n    const config = readConfig();\n    const openmodels = client(config);\n\n    if (options.interactive) {\n      await interactiveChat(openmodels, options);\n    } else if (message) {\n      await singleChat(openmodels, message, options);\n    } else {\n      console.log(chalk.red('Error: Please provide a message or use --interactive mode'));\n      process.exit(1);\n    }\n  });\n\nasync function singleChat(openmodels: any, message: string, options: any) {\n  const spinner = ora('Generating response...').start();\n  \n  try {\n    const request = {\n      model: options.model,\n      messages: [\n        { role: 'user', content: message }\n      ],\n      max_tokens: parseInt(options.maxTokens),\n      temperature: parseFloat(options.temperature),\n      stream: options.stream\n    };\n\n    if (options.stream) {\n      spinner.stop();\n      const stream = await openmodels.chat(request) as AsyncGenerator<string, void, unknown>;\n      \n      process.stdout.write(chalk.blue('Response: '));\n      for await (const token of stream) {\n        process.stdout.write(token);\n      }\n      console.log();\n    } else {\n      const response = await openmodels.chat(request);\n      spinner.stop();\n      \n      console.log(chalk.blue('Response:'));\n      console.log(response.choices[0].message.content);\n    }\n  } catch (error) {\n    spinner.stop();\n    console.error(chalk.red('Error:'), error);\n    process.exit(1);\n  }\n}\n\nasync function interactiveChat(openmodels: any, options: any) {\n  console.log(chalk.green('Interactive chat mode. Type \"exit\" to quit.'));\n  console.log(chalk.gray(`Using model: ${options.model}`));\n  \n  const readline = require('readline');\n  const rl = readline.createInterface({\n    input: process.stdin,\n    output: process.stdout\n  });\n\n  const askQuestion = () => {\n    rl.question(chalk.blue('You: '), async (input: string) => {\n      if (input.toLowerCase() === 'exit') {\n        rl.close();\n        return;\n      }\n\n      const spinner = ora('Generating response...').start();\n      \n      try {\n        const request = {\n          model: options.model,\n          messages: [\n            { role: 'user', content: input }\n          ],\n          max_tokens: parseInt(options.maxTokens),\n          temperature: parseFloat(options.temperature),\n          stream: false\n        };\n\n        const response = await openmodels.chat(request);\n        spinner.stop();\n        \n        console.log(chalk.green('AI:'), response.choices[0].message.content);\n        console.log();\n        askQuestion();\n      } catch (error) {\n        spinner.stop();\n        console.error(chalk.red('Error:'), error);\n        askQuestion();\n      }\n    });\n  };\n\n  askQuestion();\n}\n","import { HfInference } from '@huggingface/inference';\r\nimport {\r\n  ChatCompletionRequest,\r\n  EmbeddingRequest,\r\n  ImageRequest,\r\n  AudioTranscribeRequest,\r\n  ImageClassificationRequest\r\n} from '../types';\r\n\r\nexport class HuggingFaceProvider {\r\n  private hf: HfInference;\r\n  private apiKey: string;\r\n\r\n  constructor(config: { apiKey: string; hfToken?: string }) {\r\n    this.apiKey = config.apiKey;\r\n    // Use HF token if provided, otherwise use the OpenModels API key\r\n    const token = config.hfToken || process.env.HF_TOKEN;\r\n    if (!token) {\r\n      throw new Error('HuggingFace token is required');\r\n    }\r\n    this.hf = new HfInference(token);\r\n  }\r\n\r\n  async chat(request: ChatCompletionRequest): Promise<Response> {\r\n    const response = await this.hf.chatCompletion({\r\n      model: request.model,\r\n      messages: request.messages,\r\n      temperature: request.temperature,\r\n      max_tokens: request.max_tokens,\r\n      top_p: request.top_p,\r\n      stop: request.stop,\r\n      stream: request.stream || false,\r\n    });\r\n\r\n    // Convert HF response to OpenModels format\r\n    return new Response(JSON.stringify({\r\n      id: `chatcmpl-${Date.now()}`,\r\n      object: 'chat.completion',\r\n      model: request.model,\r\n      choices: [{\r\n        message: {\r\n          role: 'assistant',\r\n          content: response.choices[0].message.content\r\n        },\r\n        finish_reason: response.choices[0].finish_reason\r\n      }],\r\n      usage: response.usage\r\n    }));\r\n  }\r\n\r\n  async embed(request: EmbeddingRequest): Promise<Response> {\r\n    const response = await this.hf.featureExtraction({\r\n      model: request.model,\r\n      inputs: Array.isArray(request.input) ? request.input : [request.input]\r\n    });\r\n\r\n    return new Response(JSON.stringify({\r\n      object: 'list',\r\n      data: Array.isArray(response[0]) ? response.map((embedding, i) => ({\r\n        object: 'embedding',\r\n        embedding: embedding,\r\n        index: i\r\n      })) : [{\r\n        object: 'embedding',\r\n        embedding: response,\r\n        index: 0\r\n      }],\r\n      model: request.model\r\n    }));\r\n  }\r\n\r\n  async image(request: ImageRequest): Promise<Response> {\r\n    const response = await this.hf.textToImage({\r\n      model: request.model,\r\n      inputs: request.prompt,\r\n      parameters: {\r\n        width: parseInt(request.size?.split('x')[0] || '1024'),\r\n        height: parseInt(request.size?.split('x')[1] || '1024'),\r\n        num_inference_steps: request.quality === 'hd' ? 50 : 25\r\n      }\r\n    });\r\n\r\n    // Convert blob to base64\r\n    const buffer = await response.arrayBuffer();\r\n    const base64 = Buffer.from(buffer).toString('base64');\r\n\r\n    return new Response(JSON.stringify({\r\n      data: [{\r\n        b64_json: base64\r\n      }]\r\n    }));\r\n  }\r\n\r\n  async audioTranscribe(request: AudioTranscribeRequest): Promise<Response> {\r\n    // Fetch audio file if URL provided\r\n    const audioBlob = await fetch(request.input).then(r => r.blob());\r\n    \r\n    const response = await this.hf.automaticSpeechRecognition({\r\n      model: request.model,\r\n      data: audioBlob\r\n    });\r\n\r\n    return new Response(JSON.stringify({\r\n      text: response.text\r\n    }));\r\n  }\r\n\r\n  async audioSummarize(request: any): Promise<Response> {\r\n    // First transcribe the audio\r\n    const audioBlob = await fetch(request.input).then(r => r.blob());\r\n    \r\n    const transcription = await this.hf.automaticSpeechRecognition({\r\n      model: request.model || 'openai/whisper-base',\r\n      data: audioBlob\r\n    });\r\n\r\n    // Then summarize the text using a summarization model\r\n    const summary = await this.hf.summarization({\r\n      model: 'facebook/bart-large-cnn',\r\n      inputs: transcription.text,\r\n      parameters: {\r\n        max_length: 150,\r\n        min_length: 30\r\n      }\r\n    });\r\n\r\n    return new Response(JSON.stringify({\r\n      text: summary.summary_text\r\n    }));\r\n  }\r\n\r\n  async visionClassify(request: ImageClassificationRequest): Promise<Response> {\r\n    // Fetch image if URL provided\r\n    const imageBlob = await fetch(request.input).then(r => r.blob());\r\n    \r\n    const response = await this.hf.imageClassification({\r\n      model: request.model,\r\n      data: imageBlob\r\n    });\r\n\r\n    return new Response(JSON.stringify({\r\n      classifications: response.map(r => ({\r\n        label: r.label,\r\n        score: r.score\r\n      }))\r\n    }));\r\n  }\r\n}\r\n\r\n","import { StreamChunk } from './types';\n\nexport class OpenModelsError extends Error {\n  constructor(\n    message: string,\n    public status?: number,\n    public code?: string\n  ) {\n    super(message);\n    this.name = 'OpenModelsError';\n  }\n}\n\nexport async function* parseSSEStream(\n  response: any\n): AsyncGenerator<string, void, unknown> {\n  if (!response.body) {\n    throw new OpenModelsError('Response body is null');\n  }\n\n  // Handle authentication errors before parsing stream\n  if (response.status === 401) {\n    throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n  }\n  if (response.status === 403) {\n    throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n  }\n\n  // Handle node-fetch response\n  const text = await response.text();\n  const lines = text.split('\\n');\n\n  for (const line of lines) {\n    const trimmed = line.trim();\n    \n    if (trimmed === '') continue;\n    if (trimmed === '[DONE]') return;\n    \n    if (trimmed.startsWith('data: ')) {\n      const data = trimmed.slice(6);\n      if (data === '[DONE]') return;\n      \n      try {\n        const parsed: StreamChunk = JSON.parse(data);\n        if (parsed.choices?.[0]?.delta?.content) {\n          yield parsed.choices[0].delta.content;\n        }\n      } catch (e) {\n        // Skip malformed JSON\n        continue;\n      }\n    }\n  }\n}\n","import { Task } from './types/run';\r\n\r\nexport const TASK_TO_MODELS: Record<Task, string[]> = {\r\n  'image-classification': [\r\n    'google/vit-base-patch16-224',\r\n    'facebook/convnext-base-224',\r\n    'openai/clip-vit-base-patch32',\r\n  ],\r\n  'text-generation': [\r\n    'microsoft/DialoGPT-medium',\r\n    'meta-llama/Llama-3.1-8B-Instruct',\r\n    'meta-llama/Meta-Llama-3-8B-Instruct',\r\n  ],\r\n  'embedding': [\r\n    'sentence-transformers/all-MiniLM-L6-v2',\r\n  ],\r\n  'audio-transcribe': [\r\n    'openai/whisper-base',\r\n  ],\r\n  'audio-summarize': [\r\n    'facebook/bart-large-cnn',\r\n  ],\r\n  'image-generation': [\r\n    'runwayml/stable-diffusion-v1-5',\r\n  ],\r\n};\r\n\r\nexport function getDefaultModel(task: Task): string {\r\n  const models = TASK_TO_MODELS[task];\r\n  if (!models || models.length === 0) {\r\n    throw new Error(`No default models configured for task: ${task}`);\r\n  }\r\n  return models[0];\r\n}\r\n\r\n\r\n","import { HuggingFaceProvider } from './providers/huggingface';\r\nimport { parseSSEStream, OpenModelsError } from './streaming';\r\nimport { \r\n  OpenModelsConfig, \r\n  ChatCompletionRequest, \r\n  ChatCompletionResponse,\r\n  EmbeddingRequest,\r\n  EmbeddingResponse,\r\n  ImageRequest,\r\n  ImageResponse,\r\n  AudioTranscribeRequest,\r\n  AudioTranscribeResponse,\r\n  AudioSummarizeRequest,\r\n  AudioSummarizeResponse,\r\n  ImageClassificationRequest,\r\n  ImageClassificationResponse,\r\n  RunRequest,\r\n  RunResponse\r\n} from './types';\r\nimport { getDefaultModel } from './registry';\r\n\r\nexport class OpenModels {\r\n  private textProvider: HuggingFaceProvider;\r\n  private embedProvider: HuggingFaceProvider;\r\n  private imageProvider: HuggingFaceProvider;\r\n  private audioProvider: HuggingFaceProvider;\r\n  private visionProvider: HuggingFaceProvider;\r\n\r\n  constructor(config: OpenModelsConfig = {}) {\r\n    // Validate that API key is provided\r\n    if (!config.apiKey) {\r\n      throw new OpenModelsError('API key is required. Please provide an API key in the client configuration.');\r\n    }\r\n\r\n    // Validate API key format\r\n    if (!config.apiKey.startsWith('om_') || config.apiKey.length < 10) {\r\n      throw new OpenModelsError('Invalid API key format. API keys must start with \"om_\" and be at least 10 characters long.');\r\n    }\r\n\r\n    // Use HuggingFace Inference Providers\r\n    const hfConfig = { \r\n      apiKey: config.apiKey,\r\n      hfToken: config.hfToken \r\n    };\r\n    \r\n    this.textProvider = new HuggingFaceProvider(hfConfig);\r\n    this.embedProvider = new HuggingFaceProvider(hfConfig);\r\n    this.imageProvider = new HuggingFaceProvider(hfConfig);\r\n    this.audioProvider = new HuggingFaceProvider(hfConfig);\r\n    this.visionProvider = new HuggingFaceProvider(hfConfig);\r\n  }\r\n\r\n  async chat(request: ChatCompletionRequest): Promise<ChatCompletionResponse | AsyncGenerator<string, void, unknown>> {\r\n    try {\r\n      const response = await this.textProvider.chat(request);\r\n      \r\n      if (request.stream) {\r\n        return parseSSEStream(response);\r\n      }\r\n      \r\n      const data = await response.json();\r\n      return data as ChatCompletionResponse;\r\n    } catch (error) {\r\n      if (error instanceof Error) {\r\n        throw new OpenModelsError(error.message);\r\n      }\r\n      throw new OpenModelsError('Unknown error occurred');\r\n    }\r\n  }\r\n\r\n  async embed(request: EmbeddingRequest): Promise<EmbeddingResponse> {\r\n    try {\r\n      const response = await this.embedProvider.embed(request);\r\n      const data = await response.json();\r\n      return data as EmbeddingResponse;\r\n    } catch (error) {\r\n      if (error instanceof Error) {\r\n        throw new OpenModelsError(error.message);\r\n      }\r\n      throw new OpenModelsError('Unknown error occurred');\r\n    }\r\n  }\r\n\r\n  async image(request: ImageRequest): Promise<ImageResponse> {\r\n    try {\r\n      const response = await this.imageProvider.image(request);\r\n      const data = await response.json();\r\n      return data as ImageResponse;\r\n    } catch (error) {\r\n      if (error instanceof Error) {\r\n        throw new OpenModelsError(error.message);\r\n      }\r\n      throw new OpenModelsError('Unknown error occurred');\r\n    }\r\n  }\r\n\r\n  async run(request: RunRequest): Promise<RunResponse | AsyncGenerator<string, void, unknown>> {\r\n    try {\r\n      switch (request.task) {\r\n        case 'text-generation': {\r\n          const model = (request as any).model || getDefaultModel('text-generation');\r\n          const chatReq: ChatCompletionRequest = { ...request, model } as any;\r\n          const response = await this.textProvider.chat(chatReq);\r\n          if (chatReq.stream) return parseSSEStream(response);\r\n          return (await response.json()) as ChatCompletionResponse;\r\n        }\r\n        case 'image-generation': {\r\n          const model = (request as any).model || getDefaultModel('image-generation');\r\n          const imgReq: ImageRequest = { ...request, model } as any;\r\n          const response = await this.imageProvider.image(imgReq);\r\n          return (await response.json()) as ImageResponse;\r\n        }\r\n        case 'embedding': {\r\n          const model = (request as any).model || getDefaultModel('embedding');\r\n          const embReq: EmbeddingRequest = { ...request, model } as any;\r\n          const response = await this.embedProvider.embed(embReq);\r\n          return (await response.json()) as any;\r\n        }\r\n        case 'audio-transcribe': {\r\n          const model = (request as any).model || getDefaultModel('audio-transcribe');\r\n          const aReq: AudioTranscribeRequest = { ...request, model } as any;\r\n          const response = await this.audioProvider.audioTranscribe!(aReq);\r\n          return (await response.json()) as AudioTranscribeResponse;\r\n        }\r\n        case 'audio-summarize': {\r\n          const model = (request as any).model || getDefaultModel('audio-summarize');\r\n          const aReq: AudioSummarizeRequest = { ...request, model } as any;\r\n          const response = await this.audioProvider.audioSummarize!(aReq);\r\n          return (await response.json()) as AudioSummarizeResponse;\r\n        }\r\n        case 'image-classification': {\r\n          const model = (request as any).model || getDefaultModel('image-classification');\r\n          const vReq: ImageClassificationRequest = { ...request, model } as any;\r\n          const response = await this.visionProvider.visionClassify!(vReq);\r\n          return (await response.json()) as ImageClassificationResponse;\r\n        }\r\n        default:\r\n          throw new OpenModelsError('Unsupported task');\r\n      }\r\n    } catch (error) {\r\n      if (error instanceof Error) throw new OpenModelsError(error.message);\r\n      throw new OpenModelsError('Unknown error occurred');\r\n    }\r\n  }\r\n}\r\n\r\nexport function client(config?: OpenModelsConfig): OpenModels {\r\n  return new OpenModels(config);\r\n}\r\n\r\n","import * as fs from 'fs';\nimport * as path from 'path';\nimport * as os from 'os';\n\nexport function readConfig(): any {\n  const configPath = path.join(os.homedir(), '.openmodels', 'config.json');\n  \n  if (!fs.existsSync(configPath)) {\n    return {\n      baseUrl: 'https://modal.run/api/v1'\n    };\n  }\n  \n  try {\n    const config = JSON.parse(fs.readFileSync(configPath, 'utf8'));\n    \n    // Handle both 'base-url' and 'baseUrl' formats\n    if (config['base-url'] && !config.baseUrl) {\n      config.baseUrl = config['base-url'];\n    }\n    \n    return config;\n  } catch (error) {\n    console.log('Warning: Could not read config file, using defaults');\n    return {\n      baseUrl: 'https://modal.run/api/v1'\n    };\n  }\n}\n","import { Command } from 'commander';\r\nimport { client } from '../../src';\r\nimport chalk from 'chalk';\r\nimport ora from 'ora';\r\nimport { readConfig } from '../utils';\r\n\r\nexport const embedCommand = new Command('embed')\r\n  .description('Generate text embeddings')\r\n  .argument('<text>', 'Text to embed')\r\n  .option('-m, --model <model>', 'Embedding model to use', 'sentence-transformers/all-MiniLM-L6-v2')\r\n  .option('-f, --format <format>', 'Output format (json, values)', 'json')\r\n  .option('-u, --url <url>', 'Custom embedding backend URL')\r\n  .action(async (text, options) => {\r\n    const config = readConfig();\r\n    \r\n    // Use custom URL if provided, otherwise try to get embedding URL from config\r\n    const baseUrl = options.url || config.embedUrl || config.baseUrl;\r\n    \r\n    const openmodels = client({ baseUrl });\r\n\r\n    const spinner = ora('Generating embedding...').start();\r\n    \r\n    try {\r\n      const response = await openmodels.embed({\r\n        model: options.model,\r\n        input: text,\r\n      });\r\n\r\n      spinner.stop();\r\n\r\n      if (options.format === 'values') {\r\n        console.log(chalk.blue('Embedding values:'));\r\n        console.log(response.data[0].embedding.slice(0, 10).map((v: number) => v.toFixed(4)).join(', '));\r\n        console.log(chalk.gray(`... (${response.data[0].embedding.length} dimensions)`));\r\n      } else {\r\n        console.log(chalk.blue('Embedding response:'));\r\n        console.log(JSON.stringify(response, null, 2));\r\n      }\r\n    } catch (error) {\r\n      spinner.stop();\r\n      console.error(chalk.red('Error:'), error);\r\n      process.exit(1);\r\n    }\r\n  });\r\n","import { Command } from 'commander';\r\nimport { client } from '../../src';\r\nimport chalk from 'chalk';\r\nimport ora from 'ora';\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport { readConfig } from '../utils';\r\n\r\nexport const imageCommand = new Command('image')\r\n  .description('Generate images from text prompts')\r\n  .argument('<prompt>', 'Text prompt for image generation')\r\n  .option('-m, --model <model>', 'Image model to use', 'stabilityai/stable-diffusion-xl-base-1.0')\r\n  .option('-s, --size <size>', 'Image size', '1024x1024')\r\n  .option('-q, --quality <quality>', 'Image quality (standard, hd)', 'standard')\r\n  .option('-n, --number <number>', 'Number of images to generate', '1')\r\n  .option('-o, --output <file>', 'Output file path')\r\n  .option('-u, --url <url>', 'Custom image backend URL')\r\n  .action(async (prompt, options) => {\r\n    const config = readConfig();\r\n    \r\n    // Use custom URL if provided, otherwise try to get image URL from config\r\n    const baseUrl = options.url || config.imageUrl || config.baseUrl;\r\n    \r\n    const openmodels = client({ baseUrl });\r\n\r\n    const spinner = ora('Generating image...').start();\r\n    \r\n    try {\r\n      const response = await openmodels.image({\r\n        model: options.model,\r\n        prompt: prompt,\r\n        size: options.size,\r\n        quality: options.quality,\r\n        n: parseInt(options.number)\r\n      });\r\n\r\n      spinner.stop();\r\n\r\n      const imageData = response.data[0];\r\n      if (imageData.b64_json) {\r\n        const imageBuffer = Buffer.from(imageData.b64_json, 'base64');\r\n        \r\n        const filename = options.output || `generated_image_${Date.now()}.png`;\r\n        const filepath = path.resolve(filename);\r\n        \r\n        fs.writeFileSync(filepath, imageBuffer);\r\n        \r\n        console.log(chalk.green('✓ Image generated successfully!'));\r\n        console.log(chalk.blue('Saved to:'), filepath);\r\n        console.log(chalk.gray(`Model: ${response.model}`));\r\n        console.log(chalk.gray(`Size: ${options.size}`));\r\n      }\r\n    } catch (error) {\r\n      spinner.stop();\r\n      console.error(chalk.red('Error:'), error);\r\n      process.exit(1);\r\n    }\r\n  });\r\n","import { Command } from 'commander';\r\nimport chalk from 'chalk';\r\nimport { readConfig } from '../utils';\r\n\r\nexport const modelsCommand = new Command('models')\r\n  .description('List available models')\r\n  .option('-t, --type <type>', 'Filter by model type (text, embed, image)', 'all')\r\n  .action(async (options) => {\r\n    const config = readConfig();\r\n    \r\n    const models = {\r\n      text: [\r\n        'microsoft/DialoGPT-medium',\r\n        'microsoft/DialoGPT-large',\r\n        'facebook/blenderbot-400M-distill',\r\n        'EleutherAI/gpt-neo-2.7B',\r\n        'EleutherAI/gpt-j-6B',\r\n        'microsoft/DialoGPT-small',\r\n        'distilgpt2'\r\n      ],\r\n      embed: [\r\n        'sentence-transformers/all-MiniLM-L6-v2',\r\n        'sentence-transformers/all-mpnet-base-v2',\r\n        'BAAI/bge-large-en',\r\n        'BAAI/bge-base-en',\r\n        'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\r\n        'sentence-transformers/all-MiniLM-L12-v2'\r\n      ],\r\n      image: [\r\n        'stabilityai/stable-diffusion-xl-base-1.0',\r\n        'runwayml/stable-diffusion-v1-5',\r\n        'stabilityai/stable-diffusion-2-1',\r\n        'CompVis/stable-diffusion-v1-4'\r\n      ]\r\n    };\r\n\r\n    if (options.type === 'all') {\r\n      console.log(chalk.blue('Available Models:'));\r\n      console.log();\r\n      \r\n      console.log(chalk.green('Text Generation Models:'));\r\n      models.text.forEach(model => {\r\n        console.log(`  ${chalk.cyan('•')} ${model}`);\r\n      });\r\n      \r\n      console.log();\r\n      console.log(chalk.green('Embedding Models:'));\r\n      models.embed.forEach(model => {\r\n        console.log(`  ${chalk.cyan('•')} ${model}`);\r\n      });\r\n      \r\n      console.log();\r\n      console.log(chalk.green('Image Generation Models:'));\r\n      models.image.forEach(model => {\r\n        console.log(`  ${chalk.cyan('•')} ${model}`);\r\n      });\r\n    } else if (models[options.type as keyof typeof models]) {\r\n      console.log(chalk.blue(`${options.type.charAt(0).toUpperCase() + options.type.slice(1)} Models:`));\r\n      models[options.type as keyof typeof models].forEach(model => {\r\n        console.log(`  ${chalk.cyan('•')} ${model}`);\r\n      });\r\n    } else {\r\n      console.log(chalk.red('Error: Invalid model type. Use: text, embed, image, or all'));\r\n      process.exit(1);\r\n    }\r\n  });\r\n","import { Command } from 'commander';\r\nimport chalk from 'chalk';\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport * as os from 'os';\r\n\r\nexport const configCommand = new Command('config')\r\n  .description('Manage OpenModels configuration')\r\n  .command('set')\r\n  .description('Set configuration values')\r\n  .argument('<key>', 'Configuration key (api-key, base-url, embed-url, image-url)')\r\n  .argument('<value>', 'Configuration value')\r\n  .action(async (key, value) => {\r\n    const configPath = path.join(os.homedir(), '.openmodels', 'config.json');\r\n    const configDir = path.dirname(configPath);\r\n    \r\n    // Ensure config directory exists\r\n    if (!fs.existsSync(configDir)) {\r\n      fs.mkdirSync(configDir, { recursive: true });\r\n    }\r\n    \r\n    // Read existing config\r\n    let config: any = {};\r\n    if (fs.existsSync(configPath)) {\r\n      try {\r\n        config = JSON.parse(fs.readFileSync(configPath, 'utf8'));\r\n      } catch (error) {\r\n        console.log(chalk.yellow('Warning: Could not read existing config, creating new one'));\r\n      }\r\n    }\r\n    \r\n    // Update config\r\n    config[key] = value;\r\n    \r\n    // Write config\r\n    fs.writeFileSync(configPath, JSON.stringify(config, null, 2));\r\n    \r\n    console.log(chalk.green(`✓ Set ${key} = ${value}`));\r\n  });\r\n\r\nconfigCommand\r\n  .command('get')\r\n  .description('Get configuration values')\r\n  .argument('[key]', 'Configuration key to get (optional)')\r\n  .action(async (key) => {\r\n    const config = readConfig();\r\n    \r\n    if (key) {\r\n      if (config[key]) {\r\n        console.log(chalk.blue(`${key}:`), config[key]);\r\n      } else {\r\n        console.log(chalk.red(`Configuration key '${key}' not found`));\r\n        process.exit(1);\r\n      }\r\n    } else {\r\n      console.log(chalk.blue('Current configuration:'));\r\n      Object.entries(config).forEach(([k, v]) => {\r\n        console.log(`  ${chalk.cyan(k)}: ${v}`);\r\n      });\r\n    }\r\n  });\r\n\r\nconfigCommand\r\n  .command('list')\r\n  .description('List all configuration values')\r\n  .action(async () => {\r\n    const config = readConfig();\r\n    \r\n    console.log(chalk.blue('Current configuration:'));\r\n    Object.entries(config).forEach(([k, v]) => {\r\n      console.log(`  ${chalk.cyan(k)}: ${v}`);\r\n    });\r\n  });\r\n\r\nexport function readConfig(): any {\r\n  const configPath = path.join(os.homedir(), '.openmodels', 'config.json');\r\n  \r\n  if (!fs.existsSync(configPath)) {\r\n    return {\r\n      baseUrl: 'https://modal.run/api/v1'\r\n    };\r\n  }\r\n  \r\n  try {\r\n    return JSON.parse(fs.readFileSync(configPath, 'utf8'));\r\n  } catch (error) {\r\n    console.log(chalk.yellow('Warning: Could not read config file, using defaults'));\r\n    return {\r\n      baseUrl: 'https://modal.run/api/v1'\r\n    };\r\n  }\r\n}\r\n"],"mappings":";4dAEA,IAAAA,EAAwB,qBCFxB,IAAAC,EAAwB,qBCAxB,IAAAC,EAA4B,kCASfC,EAAN,KAA0B,CAI/B,YAAYC,EAA8C,CACxD,KAAK,OAASA,EAAO,OAErB,IAAMC,EAAQD,EAAO,SAAW,QAAQ,IAAI,SAC5C,GAAI,CAACC,EACH,MAAM,IAAI,MAAM,+BAA+B,EAEjD,KAAK,GAAK,IAAI,cAAYA,CAAK,CACjC,CAEA,MAAM,KAAKC,EAAmD,CAC5D,IAAMC,EAAW,MAAM,KAAK,GAAG,eAAe,CAC5C,MAAOD,EAAQ,MACf,SAAUA,EAAQ,SAClB,YAAaA,EAAQ,YACrB,WAAYA,EAAQ,WACpB,MAAOA,EAAQ,MACf,KAAMA,EAAQ,KACd,OAAQA,EAAQ,QAAU,EAC5B,CAAC,EAGD,OAAO,IAAI,SAAS,KAAK,UAAU,CACjC,GAAI,YAAY,KAAK,IAAI,CAAC,GAC1B,OAAQ,kBACR,MAAOA,EAAQ,MACf,QAAS,CAAC,CACR,QAAS,CACP,KAAM,YACN,QAASC,EAAS,QAAQ,CAAC,EAAE,QAAQ,OACvC,EACA,cAAeA,EAAS,QAAQ,CAAC,EAAE,aACrC,CAAC,EACD,MAAOA,EAAS,KAClB,CAAC,CAAC,CACJ,CAEA,MAAM,MAAMD,EAA8C,CACxD,IAAMC,EAAW,MAAM,KAAK,GAAG,kBAAkB,CAC/C,MAAOD,EAAQ,MACf,OAAQ,MAAM,QAAQA,EAAQ,KAAK,EAAIA,EAAQ,MAAQ,CAACA,EAAQ,KAAK,CACvE,CAAC,EAED,OAAO,IAAI,SAAS,KAAK,UAAU,CACjC,OAAQ,OACR,KAAM,MAAM,QAAQC,EAAS,CAAC,CAAC,EAAIA,EAAS,IAAI,CAACC,EAAWC,KAAO,CACjE,OAAQ,YACR,UAAWD,EACX,MAAOC,CACT,EAAE,EAAI,CAAC,CACL,OAAQ,YACR,UAAWF,EACX,MAAO,CACT,CAAC,EACD,MAAOD,EAAQ,KACjB,CAAC,CAAC,CACJ,CAEA,MAAM,MAAMA,EAA0C,CAYpD,IAAMI,EAAS,MAXE,MAAM,KAAK,GAAG,YAAY,CACzC,MAAOJ,EAAQ,MACf,OAAQA,EAAQ,OAChB,WAAY,CACV,MAAO,SAASA,EAAQ,MAAM,MAAM,GAAG,EAAE,CAAC,GAAK,MAAM,EACrD,OAAQ,SAASA,EAAQ,MAAM,MAAM,GAAG,EAAE,CAAC,GAAK,MAAM,EACtD,oBAAqBA,EAAQ,UAAY,KAAO,GAAK,EACvD,CACF,CAAC,GAG6B,YAAY,EACpCK,EAAS,OAAO,KAAKD,CAAM,EAAE,SAAS,QAAQ,EAEpD,OAAO,IAAI,SAAS,KAAK,UAAU,CACjC,KAAM,CAAC,CACL,SAAUC,CACZ,CAAC,CACH,CAAC,CAAC,CACJ,CAEA,MAAM,gBAAgBL,EAAoD,CAExE,IAAMM,EAAY,MAAM,MAAMN,EAAQ,KAAK,EAAE,KAAKO,GAAKA,EAAE,KAAK,CAAC,EAEzDN,EAAW,MAAM,KAAK,GAAG,2BAA2B,CACxD,MAAOD,EAAQ,MACf,KAAMM,CACR,CAAC,EAED,OAAO,IAAI,SAAS,KAAK,UAAU,CACjC,KAAML,EAAS,IACjB,CAAC,CAAC,CACJ,CAEA,MAAM,eAAeD,EAAiC,CAEpD,IAAMM,EAAY,MAAM,MAAMN,EAAQ,KAAK,EAAE,KAAKO,GAAKA,EAAE,KAAK,CAAC,EAEzDC,EAAgB,MAAM,KAAK,GAAG,2BAA2B,CAC7D,MAAOR,EAAQ,OAAS,sBACxB,KAAMM,CACR,CAAC,EAGKG,EAAU,MAAM,KAAK,GAAG,cAAc,CAC1C,MAAO,0BACP,OAAQD,EAAc,KACtB,WAAY,CACV,WAAY,IACZ,WAAY,EACd,CACF,CAAC,EAED,OAAO,IAAI,SAAS,KAAK,UAAU,CACjC,KAAMC,EAAQ,YAChB,CAAC,CAAC,CACJ,CAEA,MAAM,eAAeT,EAAwD,CAE3E,IAAMU,EAAY,MAAM,MAAMV,EAAQ,KAAK,EAAE,KAAKO,GAAKA,EAAE,KAAK,CAAC,EAEzDN,EAAW,MAAM,KAAK,GAAG,oBAAoB,CACjD,MAAOD,EAAQ,MACf,KAAMU,CACR,CAAC,EAED,OAAO,IAAI,SAAS,KAAK,UAAU,CACjC,gBAAiBT,EAAS,IAAIM,IAAM,CAClC,MAAOA,EAAE,MACT,MAAOA,EAAE,KACX,EAAE,CACJ,CAAC,CAAC,CACJ,CACF,ECjJO,IAAMI,EAAN,cAA8B,KAAM,CACzC,YACEC,EACOC,EACAC,EACP,CACA,MAAMF,CAAO,EAHN,YAAAC,EACA,UAAAC,EAGP,KAAK,KAAO,iBACd,CACF,EAEA,eAAuBC,EACrBC,EACuC,CACvC,GAAI,CAACA,EAAS,KACZ,MAAM,IAAIL,EAAgB,uBAAuB,EAInD,GAAIK,EAAS,SAAW,IACtB,MAAM,IAAIL,EAAgB,kDAAmD,GAAG,EAElF,GAAIK,EAAS,SAAW,IACtB,MAAM,IAAIL,EAAgB,oDAAqD,GAAG,EAKpF,IAAMM,GADO,MAAMD,EAAS,KAAK,GACd,MAAM;AAAA,CAAI,EAE7B,QAAWE,KAAQD,EAAO,CACxB,IAAME,EAAUD,EAAK,KAAK,EAE1B,GAAIC,IAAY,GAChB,IAAIA,IAAY,SAAU,OAE1B,GAAIA,EAAQ,WAAW,QAAQ,EAAG,CAChC,IAAMC,EAAOD,EAAQ,MAAM,CAAC,EAC5B,GAAIC,IAAS,SAAU,OAEvB,GAAI,CACF,IAAMC,EAAsB,KAAK,MAAMD,CAAI,EACvCC,EAAO,UAAU,CAAC,GAAG,OAAO,UAC9B,MAAMA,EAAO,QAAQ,CAAC,EAAE,MAAM,QAElC,MAAY,CAEV,QACF,CACF,EACF,CACF,CCnDO,IAAMC,GAAyC,CACpD,uBAAwB,CACtB,8BACA,6BACA,8BACF,EACA,kBAAmB,CACjB,4BACA,mCACA,qCACF,EACA,UAAa,CACX,wCACF,EACA,mBAAoB,CAClB,qBACF,EACA,kBAAmB,CACjB,yBACF,EACA,mBAAoB,CAClB,gCACF,CACF,EAEO,SAASC,EAAgBC,EAAoB,CAClD,IAAMC,EAASH,GAAeE,CAAI,EAClC,GAAI,CAACC,GAAUA,EAAO,SAAW,EAC/B,MAAM,IAAI,MAAM,0CAA0CD,CAAI,EAAE,EAElE,OAAOC,EAAO,CAAC,CACjB,CCZO,IAAMC,EAAN,KAAiB,CAOtB,YAAYC,EAA2B,CAAC,EAAG,CAEzC,GAAI,CAACA,EAAO,OACV,MAAM,IAAIC,EAAgB,6EAA6E,EAIzG,GAAI,CAACD,EAAO,OAAO,WAAW,KAAK,GAAKA,EAAO,OAAO,OAAS,GAC7D,MAAM,IAAIC,EAAgB,4FAA4F,EAIxH,IAAMC,EAAW,CACf,OAAQF,EAAO,OACf,QAASA,EAAO,OAClB,EAEA,KAAK,aAAe,IAAIG,EAAoBD,CAAQ,EACpD,KAAK,cAAgB,IAAIC,EAAoBD,CAAQ,EACrD,KAAK,cAAgB,IAAIC,EAAoBD,CAAQ,EACrD,KAAK,cAAgB,IAAIC,EAAoBD,CAAQ,EACrD,KAAK,eAAiB,IAAIC,EAAoBD,CAAQ,CACxD,CAEA,MAAM,KAAKE,EAAyG,CAClH,GAAI,CACF,IAAMC,EAAW,MAAM,KAAK,aAAa,KAAKD,CAAO,EAErD,OAAIA,EAAQ,OACHE,EAAeD,CAAQ,EAGnB,MAAMA,EAAS,KAAK,CAEnC,OAASE,EAAO,CACd,MAAIA,aAAiB,MACb,IAAIN,EAAgBM,EAAM,OAAO,EAEnC,IAAIN,EAAgB,wBAAwB,CACpD,CACF,CAEA,MAAM,MAAMG,EAAuD,CACjE,GAAI,CAGF,OADa,MADI,MAAM,KAAK,cAAc,MAAMA,CAAO,GAC3B,KAAK,CAEnC,OAASG,EAAO,CACd,MAAIA,aAAiB,MACb,IAAIN,EAAgBM,EAAM,OAAO,EAEnC,IAAIN,EAAgB,wBAAwB,CACpD,CACF,CAEA,MAAM,MAAMG,EAA+C,CACzD,GAAI,CAGF,OADa,MADI,MAAM,KAAK,cAAc,MAAMA,CAAO,GAC3B,KAAK,CAEnC,OAASG,EAAO,CACd,MAAIA,aAAiB,MACb,IAAIN,EAAgBM,EAAM,OAAO,EAEnC,IAAIN,EAAgB,wBAAwB,CACpD,CACF,CAEA,MAAM,IAAIG,EAAmF,CAC3F,GAAI,CACF,OAAQA,EAAQ,KAAM,CACpB,IAAK,kBAAmB,CACtB,IAAMI,EAASJ,EAAgB,OAASK,EAAgB,iBAAiB,EACnEC,EAAiC,CAAE,GAAGN,EAAS,MAAAI,CAAM,EACrDH,EAAW,MAAM,KAAK,aAAa,KAAKK,CAAO,EACrD,OAAIA,EAAQ,OAAeJ,EAAeD,CAAQ,EAC1C,MAAMA,EAAS,KAAK,CAC9B,CACA,IAAK,mBAAoB,CACvB,IAAMG,EAASJ,EAAgB,OAASK,EAAgB,kBAAkB,EACpEE,EAAuB,CAAE,GAAGP,EAAS,MAAAI,CAAM,EAEjD,OAAQ,MADS,MAAM,KAAK,cAAc,MAAMG,CAAM,GAC/B,KAAK,CAC9B,CACA,IAAK,YAAa,CAChB,IAAMH,EAASJ,EAAgB,OAASK,EAAgB,WAAW,EAC7DG,EAA2B,CAAE,GAAGR,EAAS,MAAAI,CAAM,EAErD,OAAQ,MADS,MAAM,KAAK,cAAc,MAAMI,CAAM,GAC/B,KAAK,CAC9B,CACA,IAAK,mBAAoB,CACvB,IAAMJ,EAASJ,EAAgB,OAASK,EAAgB,kBAAkB,EACpEI,EAA+B,CAAE,GAAGT,EAAS,MAAAI,CAAM,EAEzD,OAAQ,MADS,MAAM,KAAK,cAAc,gBAAiBK,CAAI,GACxC,KAAK,CAC9B,CACA,IAAK,kBAAmB,CACtB,IAAML,EAASJ,EAAgB,OAASK,EAAgB,iBAAiB,EACnEI,EAA8B,CAAE,GAAGT,EAAS,MAAAI,CAAM,EAExD,OAAQ,MADS,MAAM,KAAK,cAAc,eAAgBK,CAAI,GACvC,KAAK,CAC9B,CACA,IAAK,uBAAwB,CAC3B,IAAML,EAASJ,EAAgB,OAASK,EAAgB,sBAAsB,EACxEK,EAAmC,CAAE,GAAGV,EAAS,MAAAI,CAAM,EAE7D,OAAQ,MADS,MAAM,KAAK,eAAe,eAAgBM,CAAI,GACxC,KAAK,CAC9B,CACA,QACE,MAAM,IAAIb,EAAgB,kBAAkB,CAChD,CACF,OAASM,EAAO,CACd,MAAIA,aAAiB,MAAa,IAAIN,EAAgBM,EAAM,OAAO,EAC7D,IAAIN,EAAgB,wBAAwB,CACpD,CACF,CACF,EAEO,SAASc,EAAOf,EAAuC,CAC5D,OAAO,IAAID,EAAWC,CAAM,CAC9B,CJlJA,IAAAgB,EAAkB,oBAClBC,EAAgB,kBKHhB,IAAAC,EAAoB,iBACpBC,EAAsB,mBACtBC,EAAoB,iBAEb,SAASC,GAAkB,CAChC,IAAMC,EAAkB,OAAQ,UAAQ,EAAG,cAAe,aAAa,EAEvE,GAAI,CAAI,aAAWA,CAAU,EAC3B,MAAO,CACL,QAAS,0BACX,EAGF,GAAI,CACF,IAAMC,EAAS,KAAK,MAAS,eAAaD,EAAY,MAAM,CAAC,EAG7D,OAAIC,EAAO,UAAU,GAAK,CAACA,EAAO,UAChCA,EAAO,QAAUA,EAAO,UAAU,GAG7BA,CACT,MAAgB,CACd,eAAQ,IAAI,qDAAqD,EAC1D,CACL,QAAS,0BACX,CACF,CACF,CLtBO,IAAMC,EAAc,IAAI,UAAQ,MAAM,EAC1C,YAAY,qBAAqB,EACjC,SAAS,YAAa,2BAA2B,EACjD,OAAO,sBAAuB,eAAgB,2BAA2B,EACzE,OAAO,eAAgB,sBAAuB,EAAK,EACnD,OAAO,2BAA4B,6BAA8B,KAAK,EACtE,OAAO,4BAA6B,6BAA8B,KAAK,EACvE,OAAO,oBAAqB,wBAAyB,EAAK,EAC1D,OAAO,MAAOC,EAASC,IAAY,CAClC,IAAMC,EAASC,EAAW,EACpBC,EAAaC,EAAOH,CAAM,EAE5BD,EAAQ,YACV,MAAMK,GAAgBF,EAAYH,CAAO,EAChCD,EACT,MAAMO,GAAWH,EAAYJ,EAASC,CAAO,GAE7C,QAAQ,IAAI,EAAAO,QAAM,IAAI,2DAA2D,CAAC,EAClF,QAAQ,KAAK,CAAC,EAElB,CAAC,EAEH,eAAeD,GAAWH,EAAiBJ,EAAiBC,EAAc,CACxE,IAAMQ,KAAU,EAAAC,SAAI,wBAAwB,EAAE,MAAM,EAEpD,GAAI,CACF,IAAMC,EAAU,CACd,MAAOV,EAAQ,MACf,SAAU,CACR,CAAE,KAAM,OAAQ,QAASD,CAAQ,CACnC,EACA,WAAY,SAASC,EAAQ,SAAS,EACtC,YAAa,WAAWA,EAAQ,WAAW,EAC3C,OAAQA,EAAQ,MAClB,EAEA,GAAIA,EAAQ,OAAQ,CAClBQ,EAAQ,KAAK,EACb,IAAMG,EAAS,MAAMR,EAAW,KAAKO,CAAO,EAE5C,QAAQ,OAAO,MAAM,EAAAH,QAAM,KAAK,YAAY,CAAC,EAC7C,cAAiBK,KAASD,EACxB,QAAQ,OAAO,MAAMC,CAAK,EAE5B,QAAQ,IAAI,CACd,KAAO,CACL,IAAMC,EAAW,MAAMV,EAAW,KAAKO,CAAO,EAC9CF,EAAQ,KAAK,EAEb,QAAQ,IAAI,EAAAD,QAAM,KAAK,WAAW,CAAC,EACnC,QAAQ,IAAIM,EAAS,QAAQ,CAAC,EAAE,QAAQ,OAAO,CACjD,CACF,OAASC,EAAO,CACdN,EAAQ,KAAK,EACb,QAAQ,MAAM,EAAAD,QAAM,IAAI,QAAQ,EAAGO,CAAK,EACxC,QAAQ,KAAK,CAAC,CAChB,CACF,CAEA,eAAeT,GAAgBF,EAAiBH,EAAc,CAC5D,QAAQ,IAAI,EAAAO,QAAM,MAAM,6CAA6C,CAAC,EACtE,QAAQ,IAAI,EAAAA,QAAM,KAAK,gBAAgBP,EAAQ,KAAK,EAAE,CAAC,EAGvD,IAAMe,EADW,QAAQ,UAAU,EACf,gBAAgB,CAClC,MAAO,QAAQ,MACf,OAAQ,QAAQ,MAClB,CAAC,EAEKC,EAAc,IAAM,CACxBD,EAAG,SAAS,EAAAR,QAAM,KAAK,OAAO,EAAG,MAAOU,GAAkB,CACxD,GAAIA,EAAM,YAAY,IAAM,OAAQ,CAClCF,EAAG,MAAM,EACT,MACF,CAEA,IAAMP,KAAU,EAAAC,SAAI,wBAAwB,EAAE,MAAM,EAEpD,GAAI,CACF,IAAMC,EAAU,CACd,MAAOV,EAAQ,MACf,SAAU,CACR,CAAE,KAAM,OAAQ,QAASiB,CAAM,CACjC,EACA,WAAY,SAASjB,EAAQ,SAAS,EACtC,YAAa,WAAWA,EAAQ,WAAW,EAC3C,OAAQ,EACV,EAEMa,EAAW,MAAMV,EAAW,KAAKO,CAAO,EAC9CF,EAAQ,KAAK,EAEb,QAAQ,IAAI,EAAAD,QAAM,MAAM,KAAK,EAAGM,EAAS,QAAQ,CAAC,EAAE,QAAQ,OAAO,EACnE,QAAQ,IAAI,EACZG,EAAY,CACd,OAASF,EAAO,CACdN,EAAQ,KAAK,EACb,QAAQ,MAAM,EAAAD,QAAM,IAAI,QAAQ,EAAGO,CAAK,EACxCE,EAAY,CACd,CACF,CAAC,CACH,EAEAA,EAAY,CACd,CM9GA,IAAAE,EAAwB,qBAExB,IAAAC,EAAkB,oBAClBC,EAAgB,kBAGT,IAAMC,EAAe,IAAI,UAAQ,OAAO,EAC5C,YAAY,0BAA0B,EACtC,SAAS,SAAU,eAAe,EAClC,OAAO,sBAAuB,yBAA0B,wCAAwC,EAChG,OAAO,wBAAyB,+BAAgC,MAAM,EACtE,OAAO,kBAAmB,8BAA8B,EACxD,OAAO,MAAOC,EAAMC,IAAY,CAC/B,IAAMC,EAASC,EAAW,EAGpBC,EAAUH,EAAQ,KAAOC,EAAO,UAAYA,EAAO,QAEnDG,EAAaC,EAAO,CAAE,QAAAF,CAAQ,CAAC,EAE/BG,KAAU,EAAAC,SAAI,yBAAyB,EAAE,MAAM,EAErD,GAAI,CACF,IAAMC,EAAW,MAAMJ,EAAW,MAAM,CACtC,MAAOJ,EAAQ,MACf,MAAOD,CACT,CAAC,EAEDO,EAAQ,KAAK,EAETN,EAAQ,SAAW,UACrB,QAAQ,IAAI,EAAAS,QAAM,KAAK,mBAAmB,CAAC,EAC3C,QAAQ,IAAID,EAAS,KAAK,CAAC,EAAE,UAAU,MAAM,EAAG,EAAE,EAAE,IAAKE,GAAcA,EAAE,QAAQ,CAAC,CAAC,EAAE,KAAK,IAAI,CAAC,EAC/F,QAAQ,IAAI,EAAAD,QAAM,KAAK,QAAQD,EAAS,KAAK,CAAC,EAAE,UAAU,MAAM,cAAc,CAAC,IAE/E,QAAQ,IAAI,EAAAC,QAAM,KAAK,qBAAqB,CAAC,EAC7C,QAAQ,IAAI,KAAK,UAAUD,EAAU,KAAM,CAAC,CAAC,EAEjD,OAASG,EAAO,CACdL,EAAQ,KAAK,EACb,QAAQ,MAAM,EAAAG,QAAM,IAAI,QAAQ,EAAGE,CAAK,EACxC,QAAQ,KAAK,CAAC,CAChB,CACF,CAAC,EC3CH,IAAAC,EAAwB,qBAExB,IAAAC,EAAkB,oBAClBC,EAAgB,kBAChBC,EAAoB,iBACpBC,EAAsB,mBAGf,IAAMC,EAAe,IAAI,UAAQ,OAAO,EAC5C,YAAY,mCAAmC,EAC/C,SAAS,WAAY,kCAAkC,EACvD,OAAO,sBAAuB,qBAAsB,0CAA0C,EAC9F,OAAO,oBAAqB,aAAc,WAAW,EACrD,OAAO,0BAA2B,+BAAgC,UAAU,EAC5E,OAAO,wBAAyB,+BAAgC,GAAG,EACnE,OAAO,sBAAuB,kBAAkB,EAChD,OAAO,kBAAmB,0BAA0B,EACpD,OAAO,MAAOC,EAAQC,IAAY,CACjC,IAAMC,EAASC,EAAW,EAGpBC,EAAUH,EAAQ,KAAOC,EAAO,UAAYA,EAAO,QAEnDG,EAAaC,EAAO,CAAE,QAAAF,CAAQ,CAAC,EAE/BG,KAAU,EAAAC,SAAI,qBAAqB,EAAE,MAAM,EAEjD,GAAI,CACF,IAAMC,EAAW,MAAMJ,EAAW,MAAM,CACtC,MAAOJ,EAAQ,MACf,OAAQD,EACR,KAAMC,EAAQ,KACd,QAASA,EAAQ,QACjB,EAAG,SAASA,EAAQ,MAAM,CAC5B,CAAC,EAEDM,EAAQ,KAAK,EAEb,IAAMG,EAAYD,EAAS,KAAK,CAAC,EACjC,GAAIC,EAAU,SAAU,CACtB,IAAMC,EAAc,OAAO,KAAKD,EAAU,SAAU,QAAQ,EAEtDE,EAAWX,EAAQ,QAAU,mBAAmB,KAAK,IAAI,CAAC,OAC1DY,EAAgB,UAAQD,CAAQ,EAEnC,gBAAcC,EAAUF,CAAW,EAEtC,QAAQ,IAAI,EAAAG,QAAM,MAAM,sCAAiC,CAAC,EAC1D,QAAQ,IAAI,EAAAA,QAAM,KAAK,WAAW,EAAGD,CAAQ,EAC7C,QAAQ,IAAI,EAAAC,QAAM,KAAK,UAAUL,EAAS,KAAK,EAAE,CAAC,EAClD,QAAQ,IAAI,EAAAK,QAAM,KAAK,SAASb,EAAQ,IAAI,EAAE,CAAC,CACjD,CACF,OAASc,EAAO,CACdR,EAAQ,KAAK,EACb,QAAQ,MAAM,EAAAO,QAAM,IAAI,QAAQ,EAAGC,CAAK,EACxC,QAAQ,KAAK,CAAC,CAChB,CACF,CAAC,ECzDH,IAAAC,EAAwB,qBACxBC,EAAkB,oBAGX,IAAMC,EAAgB,IAAI,UAAQ,QAAQ,EAC9C,YAAY,uBAAuB,EACnC,OAAO,oBAAqB,4CAA6C,KAAK,EAC9E,OAAO,MAAOC,GAAY,CACzB,IAAMC,EAASC,EAAW,EAEpBC,EAAS,CACb,KAAM,CACJ,4BACA,2BACA,mCACA,0BACA,sBACA,2BACA,YACF,EACA,MAAO,CACL,yCACA,0CACA,oBACA,mBACA,8DACA,yCACF,EACA,MAAO,CACL,2CACA,iCACA,mCACA,+BACF,CACF,EAEIH,EAAQ,OAAS,OACnB,QAAQ,IAAI,EAAAI,QAAM,KAAK,mBAAmB,CAAC,EAC3C,QAAQ,IAAI,EAEZ,QAAQ,IAAI,EAAAA,QAAM,MAAM,yBAAyB,CAAC,EAClDD,EAAO,KAAK,QAAQE,GAAS,CAC3B,QAAQ,IAAI,KAAK,EAAAD,QAAM,KAAK,QAAG,CAAC,IAAIC,CAAK,EAAE,CAC7C,CAAC,EAED,QAAQ,IAAI,EACZ,QAAQ,IAAI,EAAAD,QAAM,MAAM,mBAAmB,CAAC,EAC5CD,EAAO,MAAM,QAAQE,GAAS,CAC5B,QAAQ,IAAI,KAAK,EAAAD,QAAM,KAAK,QAAG,CAAC,IAAIC,CAAK,EAAE,CAC7C,CAAC,EAED,QAAQ,IAAI,EACZ,QAAQ,IAAI,EAAAD,QAAM,MAAM,0BAA0B,CAAC,EACnDD,EAAO,MAAM,QAAQE,GAAS,CAC5B,QAAQ,IAAI,KAAK,EAAAD,QAAM,KAAK,QAAG,CAAC,IAAIC,CAAK,EAAE,CAC7C,CAAC,GACQF,EAAOH,EAAQ,IAA2B,GACnD,QAAQ,IAAI,EAAAI,QAAM,KAAK,GAAGJ,EAAQ,KAAK,OAAO,CAAC,EAAE,YAAY,EAAIA,EAAQ,KAAK,MAAM,CAAC,CAAC,UAAU,CAAC,EACjGG,EAAOH,EAAQ,IAA2B,EAAE,QAAQK,GAAS,CAC3D,QAAQ,IAAI,KAAK,EAAAD,QAAM,KAAK,QAAG,CAAC,IAAIC,CAAK,EAAE,CAC7C,CAAC,IAED,QAAQ,IAAI,EAAAD,QAAM,IAAI,4DAA4D,CAAC,EACnF,QAAQ,KAAK,CAAC,EAElB,CAAC,ECjEH,IAAAE,EAAwB,qBACxBC,EAAkB,oBAClBC,EAAoB,iBACpBC,EAAsB,mBACtBC,EAAoB,iBAEPC,EAAgB,IAAI,UAAQ,QAAQ,EAC9C,YAAY,iCAAiC,EAC7C,QAAQ,KAAK,EACb,YAAY,0BAA0B,EACtC,SAAS,QAAS,6DAA6D,EAC/E,SAAS,UAAW,qBAAqB,EACzC,OAAO,MAAOC,EAAKC,IAAU,CAC5B,IAAMC,EAAkB,OAAQ,UAAQ,EAAG,cAAe,aAAa,EACjEC,EAAiB,UAAQD,CAAU,EAGjC,aAAWC,CAAS,GACvB,YAAUA,EAAW,CAAE,UAAW,EAAK,CAAC,EAI7C,IAAIC,EAAc,CAAC,EACnB,GAAO,aAAWF,CAAU,EAC1B,GAAI,CACFE,EAAS,KAAK,MAAS,eAAaF,EAAY,MAAM,CAAC,CACzD,MAAgB,CACd,QAAQ,IAAI,EAAAG,QAAM,OAAO,2DAA2D,CAAC,CACvF,CAIFD,EAAOJ,CAAG,EAAIC,EAGX,gBAAcC,EAAY,KAAK,UAAUE,EAAQ,KAAM,CAAC,CAAC,EAE5D,QAAQ,IAAI,EAAAC,QAAM,MAAM,cAASL,CAAG,MAAMC,CAAK,EAAE,CAAC,CACpD,CAAC,EAEHF,EACG,QAAQ,KAAK,EACb,YAAY,0BAA0B,EACtC,SAAS,QAAS,qCAAqC,EACvD,OAAO,MAAOC,GAAQ,CACrB,IAAMI,EAASE,EAAW,EAEtBN,EACEI,EAAOJ,CAAG,EACZ,QAAQ,IAAI,EAAAK,QAAM,KAAK,GAAGL,CAAG,GAAG,EAAGI,EAAOJ,CAAG,CAAC,GAE9C,QAAQ,IAAI,EAAAK,QAAM,IAAI,sBAAsBL,CAAG,aAAa,CAAC,EAC7D,QAAQ,KAAK,CAAC,IAGhB,QAAQ,IAAI,EAAAK,QAAM,KAAK,wBAAwB,CAAC,EAChD,OAAO,QAAQD,CAAM,EAAE,QAAQ,CAAC,CAACG,EAAGC,CAAC,IAAM,CACzC,QAAQ,IAAI,KAAK,EAAAH,QAAM,KAAKE,CAAC,CAAC,KAAKC,CAAC,EAAE,CACxC,CAAC,EAEL,CAAC,EAEHT,EACG,QAAQ,MAAM,EACd,YAAY,+BAA+B,EAC3C,OAAO,SAAY,CAClB,IAAMK,EAASE,EAAW,EAE1B,QAAQ,IAAI,EAAAD,QAAM,KAAK,wBAAwB,CAAC,EAChD,OAAO,QAAQD,CAAM,EAAE,QAAQ,CAAC,CAACG,EAAGC,CAAC,IAAM,CACzC,QAAQ,IAAI,KAAK,EAAAH,QAAM,KAAKE,CAAC,CAAC,KAAKC,CAAC,EAAE,CACxC,CAAC,CACH,CAAC,EAEI,SAASF,GAAkB,CAChC,IAAMJ,EAAkB,OAAQ,UAAQ,EAAG,cAAe,aAAa,EAEvE,GAAI,CAAI,aAAWA,CAAU,EAC3B,MAAO,CACL,QAAS,0BACX,EAGF,GAAI,CACF,OAAO,KAAK,MAAS,eAAaA,EAAY,MAAM,CAAC,CACvD,MAAgB,CACd,eAAQ,IAAI,EAAAG,QAAM,OAAO,qDAAqD,CAAC,EACxE,CACL,QAAS,0BACX,CACF,CACF,CVlFA,IAAMI,EAAU,IAAI,UAEpBA,EACG,KAAK,YAAY,EACjB,YAAY,gDAAgD,EAC5D,QAAQ,OAAO,EAGlBA,EAAQ,WAAWC,CAAW,EAC9BD,EAAQ,WAAWE,CAAY,EAC/BF,EAAQ,WAAWG,CAAY,EAC/BH,EAAQ,WAAWI,CAAa,EAChCJ,EAAQ,WAAWK,CAAa,EAEhCL,EAAQ,MAAM","names":["import_commander","import_commander","import_inference","HuggingFaceProvider","config","token","request","response","embedding","i","buffer","base64","audioBlob","r","transcription","summary","imageBlob","OpenModelsError","message","status","code","parseSSEStream","response","lines","line","trimmed","data","parsed","TASK_TO_MODELS","getDefaultModel","task","models","OpenModels","config","OpenModelsError","hfConfig","HuggingFaceProvider","request","response","parseSSEStream","error","model","getDefaultModel","chatReq","imgReq","embReq","aReq","vReq","client","import_chalk","import_ora","fs","path","os","readConfig","configPath","config","chatCommand","message","options","config","readConfig","openmodels","client","interactiveChat","singleChat","chalk","spinner","ora","request","stream","token","response","error","rl","askQuestion","input","import_commander","import_chalk","import_ora","embedCommand","text","options","config","readConfig","baseUrl","openmodels","client","spinner","ora","response","chalk","v","error","import_commander","import_chalk","import_ora","fs","path","imageCommand","prompt","options","config","readConfig","baseUrl","openmodels","client","spinner","ora","response","imageData","imageBuffer","filename","filepath","chalk","error","import_commander","import_chalk","modelsCommand","options","config","readConfig","models","chalk","model","import_commander","import_chalk","fs","path","os","configCommand","key","value","configPath","configDir","config","chalk","readConfig","k","v","program","chatCommand","embedCommand","imageCommand","modelsCommand","configCommand"]}