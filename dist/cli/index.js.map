{"version":3,"sources":["../../cli/index.ts","../../cli/commands/chat.ts","../../src/providers/modal.ts","../../src/providers/base.ts","../../src/streaming.ts","../../src/registry.ts","../../src/client.ts","../../cli/utils.ts","../../cli/commands/embed.ts","../../cli/commands/image.ts","../../cli/commands/models.ts","../../cli/commands/config.ts"],"sourcesContent":["#!/usr/bin/env node\r\n\r\nimport { Command } from 'commander';\r\nimport { chatCommand } from './commands/chat';\r\nimport { embedCommand } from './commands/embed';\r\nimport { imageCommand } from './commands/image';\r\nimport { modelsCommand } from './commands/models';\r\nimport { configCommand } from './commands/config';\r\n\r\nconst program = new Command();\r\n\r\nprogram\r\n  .name('openmodels')\r\n  .description('CLI for OpenModels - Open-source AI models SDK')\r\n  .version('0.3.0');\r\n\r\n// Add commands\r\nprogram.addCommand(chatCommand);\r\nprogram.addCommand(embedCommand);\r\nprogram.addCommand(imageCommand);\r\nprogram.addCommand(modelsCommand);\r\nprogram.addCommand(configCommand);\r\n\r\nprogram.parse();\r\n","import { Command } from 'commander';\nimport { client } from '../../src';\nimport chalk from 'chalk';\nimport ora from 'ora';\nimport { readConfig } from '../utils';\n\nexport const chatCommand = new Command('chat')\n  .description('Chat with AI models')\n  .argument('[message]', 'Message to send to the AI')\n  .option('-m, --model <model>', 'Model to use', 'microsoft/DialoGPT-medium')\n  .option('-s, --stream', 'Stream the response', false)\n  .option('-t, --temperature <temp>', 'Temperature for generation', '0.7')\n  .option('-k, --max-tokens <tokens>', 'Maximum tokens to generate', '200')\n  .option('-i, --interactive', 'Interactive chat mode', false)\n  .action(async (message, options) => {\n    const config = readConfig();\n    const openmodels = client(config);\n\n    if (options.interactive) {\n      await interactiveChat(openmodels, options);\n    } else if (message) {\n      await singleChat(openmodels, message, options);\n    } else {\n      console.log(chalk.red('Error: Please provide a message or use --interactive mode'));\n      process.exit(1);\n    }\n  });\n\nasync function singleChat(openmodels: any, message: string, options: any) {\n  const spinner = ora('Generating response...').start();\n  \n  try {\n    const request = {\n      model: options.model,\n      messages: [\n        { role: 'user', content: message }\n      ],\n      max_tokens: parseInt(options.maxTokens),\n      temperature: parseFloat(options.temperature),\n      stream: options.stream\n    };\n\n    if (options.stream) {\n      spinner.stop();\n      const stream = await openmodels.chat(request) as AsyncGenerator<string, void, unknown>;\n      \n      process.stdout.write(chalk.blue('Response: '));\n      for await (const token of stream) {\n        process.stdout.write(token);\n      }\n      console.log();\n    } else {\n      const response = await openmodels.chat(request);\n      spinner.stop();\n      \n      console.log(chalk.blue('Response:'));\n      console.log(response.choices[0].message.content);\n    }\n  } catch (error) {\n    spinner.stop();\n    console.error(chalk.red('Error:'), error);\n    process.exit(1);\n  }\n}\n\nasync function interactiveChat(openmodels: any, options: any) {\n  console.log(chalk.green('Interactive chat mode. Type \"exit\" to quit.'));\n  console.log(chalk.gray(`Using model: ${options.model}`));\n  \n  const readline = require('readline');\n  const rl = readline.createInterface({\n    input: process.stdin,\n    output: process.stdout\n  });\n\n  const askQuestion = () => {\n    rl.question(chalk.blue('You: '), async (input: string) => {\n      if (input.toLowerCase() === 'exit') {\n        rl.close();\n        return;\n      }\n\n      const spinner = ora('Generating response...').start();\n      \n      try {\n        const request = {\n          model: options.model,\n          messages: [\n            { role: 'user', content: input }\n          ],\n          max_tokens: parseInt(options.maxTokens),\n          temperature: parseFloat(options.temperature),\n          stream: false\n        };\n\n        const response = await openmodels.chat(request);\n        spinner.stop();\n        \n        console.log(chalk.green('AI:'), response.choices[0].message.content);\n        console.log();\n        askQuestion();\n      } catch (error) {\n        spinner.stop();\n        console.error(chalk.red('Error:'), error);\n        askQuestion();\n      }\n    });\n  };\n\n  askQuestion();\n}\n","import fetch from 'node-fetch';\nimport { BaseProvider } from './base';\nimport { ChatCompletionRequest, EmbeddingRequest, ImageRequest, AudioTranscribeRequest, AudioSummarizeRequest, ImageClassificationRequest } from '../types';\nimport { OpenModelsError } from '../streaming';\n\nexport class ModalProvider extends BaseProvider {\n  async chat(request: ChatCompletionRequest): Promise<any> {\n    const url = `${this.config.baseUrl}/chat`;\n    \n    const headers: Record<string, string> = {\n      'Content-Type': 'application/json',\n      'Authorization': `Bearer ${this.config.apiKey}`,\n    };\n\n    const response = await fetch(url, {\n      method: 'POST',\n      headers,\n      body: JSON.stringify(request),\n    });\n\n    if (!response.ok) {\n      const errorText = await response.text();\n      \n      if (response.status === 401) {\n        throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n      }\n      if (response.status === 403) {\n        throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n      }\n      \n      throw new OpenModelsError(`Modal API error: ${response.status} ${errorText}`, response.status);\n    }\n\n    return response;\n  }\n\n  async embed(request: EmbeddingRequest): Promise<any> {\n    const url = `${this.config.baseUrl}/embed`;\n    \n    const headers: Record<string, string> = {\n      'Content-Type': 'application/json',\n      'Authorization': `Bearer ${this.config.apiKey}`,\n    };\n\n    const response = await fetch(url, {\n      method: 'POST',\n      headers,\n      body: JSON.stringify(request),\n    });\n\n    if (!response.ok) {\n      const errorText = await response.text();\n      \n      if (response.status === 401) {\n        throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n      }\n      if (response.status === 403) {\n        throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n      }\n      \n      throw new OpenModelsError(`Modal API error: ${response.status} ${errorText}`, response.status);\n    }\n\n    return response;\n  }\n\n  async image(request: ImageRequest): Promise<any> {\n    const url = `${this.config.baseUrl}/image`;\n    \n    const headers: Record<string, string> = {\n      'Content-Type': 'application/json',\n      'Authorization': `Bearer ${this.config.apiKey}`,\n    };\n\n    const response = await fetch(url, {\n      method: 'POST',\n      headers,\n      body: JSON.stringify(request),\n    });\n\n    if (!response.ok) {\n      const errorText = await response.text();\n      \n      if (response.status === 401) {\n        throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n      }\n      if (response.status === 403) {\n        throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n      }\n      \n      throw new OpenModelsError(`Modal API error: ${response.status} ${errorText}`, response.status);\n    }\n\n    return response;\n  }\n\n  async audioTranscribe(request: AudioTranscribeRequest): Promise<any> {\n    const url = `${this.config.baseUrl}/transcribe`;\n    const headers: Record<string, string> = {\n      'Content-Type': 'application/json',\n      'Authorization': `Bearer ${this.config.apiKey}`,\n    };\n    const response = await fetch(url, { method: 'POST', headers, body: JSON.stringify(request) });\n    if (!response.ok) {\n      const errorText = await response.text();\n      if (response.status === 401) throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n      if (response.status === 403) throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n      throw new OpenModelsError(`Modal API error: ${response.status} ${errorText}`, response.status);\n    }\n    return response;\n  }\n\n  async audioSummarize(request: AudioSummarizeRequest): Promise<any> {\n    const url = `${this.config.baseUrl}/summarize`;\n    const headers: Record<string, string> = {\n      'Content-Type': 'application/json',\n      'Authorization': `Bearer ${this.config.apiKey}`,\n    };\n    const response = await fetch(url, { method: 'POST', headers, body: JSON.stringify(request) });\n    if (!response.ok) {\n      const errorText = await response.text();\n      if (response.status === 401) throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n      if (response.status === 403) throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n      throw new OpenModelsError(`Modal API error: ${response.status} ${errorText}`, response.status);\n    }\n    return response;\n  }\n\n  async visionClassify(request: ImageClassificationRequest): Promise<any> {\n    const url = `${this.config.baseUrl}/classify`;\n    const headers: Record<string, string> = {\n      'Content-Type': 'application/json',\n      'Authorization': `Bearer ${this.config.apiKey}`,\n    };\n    const response = await fetch(url, { method: 'POST', headers, body: JSON.stringify(request) });\n    if (!response.ok) {\n      const errorText = await response.text();\n      if (response.status === 401) throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n      if (response.status === 403) throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n      throw new OpenModelsError(`Modal API error: ${response.status} ${errorText}`, response.status);\n    }\n    return response;\n  }\n}\n","import { ProviderConfig } from '../types';\nimport { AudioSummarizeRequest, AudioTranscribeRequest, ImageClassificationRequest } from '../types';\n\nexport interface Provider {\n  chat(request: any): Promise<any>;\n  embed(request: any): Promise<any>;\n  image(request: any): Promise<any>;\n  audioTranscribe?(request: AudioTranscribeRequest): Promise<any>;\n  audioSummarize?(request: AudioSummarizeRequest): Promise<any>;\n  visionClassify?(request: ImageClassificationRequest): Promise<any>;\n}\n\nexport abstract class BaseProvider implements Provider {\n  constructor(protected config: ProviderConfig) {}\n  \n  abstract chat(request: any): Promise<any>;\n  abstract embed(request: any): Promise<any>;\n  abstract image(request: any): Promise<any>;\n  audioTranscribe?(request: AudioTranscribeRequest): Promise<any>;\n  audioSummarize?(request: AudioSummarizeRequest): Promise<any>;\n  visionClassify?(request: ImageClassificationRequest): Promise<any>;\n}\n","import { StreamChunk } from './types';\n\nexport class OpenModelsError extends Error {\n  constructor(\n    message: string,\n    public status?: number,\n    public code?: string\n  ) {\n    super(message);\n    this.name = 'OpenModelsError';\n  }\n}\n\nexport async function* parseSSEStream(\n  response: any\n): AsyncGenerator<string, void, unknown> {\n  if (!response.body) {\n    throw new OpenModelsError('Response body is null');\n  }\n\n  // Handle authentication errors before parsing stream\n  if (response.status === 401) {\n    throw new OpenModelsError('Invalid API key. Please check your credentials.', 401);\n  }\n  if (response.status === 403) {\n    throw new OpenModelsError('Insufficient credits. Please top up your account.', 403);\n  }\n\n  // Handle node-fetch response\n  const text = await response.text();\n  const lines = text.split('\\n');\n\n  for (const line of lines) {\n    const trimmed = line.trim();\n    \n    if (trimmed === '') continue;\n    if (trimmed === '[DONE]') return;\n    \n    if (trimmed.startsWith('data: ')) {\n      const data = trimmed.slice(6);\n      if (data === '[DONE]') return;\n      \n      try {\n        const parsed: StreamChunk = JSON.parse(data);\n        if (parsed.choices?.[0]?.delta?.content) {\n          yield parsed.choices[0].delta.content;\n        }\n      } catch (e) {\n        // Skip malformed JSON\n        continue;\n      }\n    }\n  }\n}\n","import { Task } from './types/run';\r\n\r\nexport const TASK_TO_MODELS: Record<Task, string[]> = {\r\n  'image-classification': [\r\n    'google/vit-base-patch16-224',\r\n    'facebook/convnext-base-224',\r\n    'openai/clip-vit-base-patch32',\r\n  ],\r\n  'text-generation': [\r\n    'microsoft/DialoGPT-medium',\r\n    'meta-llama/Llama-3.1-8B-Instruct',\r\n  ],\r\n  'embedding': [\r\n    'sentence-transformers/all-MiniLM-L6-v2',\r\n  ],\r\n  'audio-transcribe': [\r\n    'openai/whisper-base',\r\n  ],\r\n  'audio-summarize': [\r\n    'facebook/bart-large-cnn',\r\n  ],\r\n  'image-generation': [\r\n    'runwayml/stable-diffusion-v1-5',\r\n  ],\r\n};\r\n\r\nexport function getDefaultModel(task: Task): string {\r\n  const models = TASK_TO_MODELS[task];\r\n  if (!models || models.length === 0) {\r\n    throw new Error(`No default models configured for task: ${task}`);\r\n  }\r\n  return models[0];\r\n}\r\n\r\n\r\n","import { ModalProvider } from './providers/modal';\nimport { parseSSEStream, OpenModelsError } from './streaming';\nimport { \n  OpenModelsConfig, \n  ChatCompletionRequest, \n  ChatCompletionResponse,\n  EmbeddingRequest,\n  EmbeddingResponse,\n  ImageRequest,\n  ImageResponse,\n  AudioTranscribeRequest,\n  AudioTranscribeResponse,\n  AudioSummarizeRequest,\n  AudioSummarizeResponse,\n  ImageClassificationRequest,\n  ImageClassificationResponse,\n  RunRequest,\n  RunResponse\n} from './types';\nimport { getDefaultModel } from './registry';\n\nexport class OpenModels {\n  private textProvider: ModalProvider;\n  private embedProvider: ModalProvider;\n  private imageProvider: ModalProvider;\n  private audioProvider: ModalProvider;\n  private visionProvider: ModalProvider;\n\n  constructor(config: OpenModelsConfig = {}) {\n    // Validate that API key is provided\n    if (!config.apiKey) {\n      throw new OpenModelsError('API key is required. Please provide an API key in the client configuration.');\n    }\n\n    // Validate API key format\n    if (!config.apiKey.startsWith('om_') || config.apiKey.length < 10) {\n      throw new OpenModelsError('Invalid API key format. API keys must start with \"om_\" and be at least 10 characters long.');\n    }\n\n    // Determine base URLs for each service\n    const baseUrl = config.baseUrl || 'https://tryscout.dev';\n    \n    // If baseUrl contains modal.run or is a full URL, use as-is\n    // Otherwise, assume it's a base domain and add subfolders\n    const getServiceUrl = (service: string) => {\n      if (baseUrl.includes('modal.run') || baseUrl.includes('/api/')) {\n        // If it's already a full URL or contains subfolder, use as-is\n        return baseUrl;\n      } else if (baseUrl.includes('.')) {\n        // If it's a domain, add subfolder\n        return `${baseUrl}/api/${service}`;\n      } else {\n        // Default to tryscout.dev subfolders\n        return `https://tryscout.dev/api/${service}`;\n      }\n    };\n\n    this.textProvider = new ModalProvider({\n      apiKey: config.apiKey,\n      baseUrl: getServiceUrl('text'),\n    });\n\n    this.embedProvider = new ModalProvider({\n      apiKey: config.apiKey,\n      baseUrl: getServiceUrl('embed'),\n    });\n\n    this.imageProvider = new ModalProvider({\n      apiKey: config.apiKey,\n      baseUrl: getServiceUrl('image'),\n    });\n\n    this.audioProvider = new ModalProvider({\n      apiKey: config.apiKey,\n      baseUrl: getServiceUrl('audio'),\n    });\n\n    this.visionProvider = new ModalProvider({\n      apiKey: config.apiKey,\n      baseUrl: getServiceUrl('vision'),\n    });\n  }\n\n  async chat(request: ChatCompletionRequest): Promise<ChatCompletionResponse | AsyncGenerator<string, void, unknown>> {\n    try {\n      const response = await this.textProvider.chat(request);\n      \n      if (request.stream) {\n        return parseSSEStream(response);\n      }\n      \n      const data = await response.json();\n      return data as ChatCompletionResponse;\n    } catch (error) {\n      if (error instanceof Error) {\n        throw new OpenModelsError(error.message);\n      }\n      throw new OpenModelsError('Unknown error occurred');\n    }\n  }\n\n  async embed(request: EmbeddingRequest): Promise<EmbeddingResponse> {\n    try {\n      const response = await this.embedProvider.embed(request);\n      const data = await response.json();\n      return data as EmbeddingResponse;\n    } catch (error) {\n      if (error instanceof Error) {\n        throw new OpenModelsError(error.message);\n      }\n      throw new OpenModelsError('Unknown error occurred');\n    }\n  }\n\n  async image(request: ImageRequest): Promise<ImageResponse> {\n    try {\n      const response = await this.imageProvider.image(request);\n      const data = await response.json();\n      return data as ImageResponse;\n    } catch (error) {\n      if (error instanceof Error) {\n        throw new OpenModelsError(error.message);\n      }\n      throw new OpenModelsError('Unknown error occurred');\n    }\n  }\n\n  async run(request: RunRequest): Promise<RunResponse | AsyncGenerator<string, void, unknown>> {\n    try {\n      switch (request.task) {\n        case 'text-generation': {\n          const model = (request as any).model || getDefaultModel('text-generation');\n          const chatReq: ChatCompletionRequest = { ...request, model } as any;\n          const response = await this.textProvider.chat(chatReq);\n          if (chatReq.stream) return parseSSEStream(response);\n          return (await response.json()) as ChatCompletionResponse;\n        }\n        case 'image-generation': {\n          const model = (request as any).model || getDefaultModel('image-generation');\n          const imgReq: ImageRequest = { ...request, model } as any;\n          const response = await this.imageProvider.image(imgReq);\n          return (await response.json()) as ImageResponse;\n        }\n        case 'embedding': {\n          const model = (request as any).model || getDefaultModel('embedding');\n          const embReq: EmbeddingRequest = { ...request, model } as any;\n          const response = await this.embedProvider.embed(embReq);\n          return (await response.json()) as any;\n        }\n        case 'audio-transcribe': {\n          const model = (request as any).model || getDefaultModel('audio-transcribe');\n          const aReq: AudioTranscribeRequest = { ...request, model } as any;\n          const response = await this.audioProvider.audioTranscribe!(aReq);\n          return (await response.json()) as AudioTranscribeResponse;\n        }\n        case 'audio-summarize': {\n          const model = (request as any).model || getDefaultModel('audio-summarize');\n          const aReq: AudioSummarizeRequest = { ...request, model } as any;\n          const response = await this.audioProvider.audioSummarize!(aReq);\n          return (await response.json()) as AudioSummarizeResponse;\n        }\n        case 'image-classification': {\n          const model = (request as any).model || getDefaultModel('image-classification');\n          const vReq: ImageClassificationRequest = { ...request, model } as any;\n          const response = await this.visionProvider.visionClassify!(vReq);\n          return (await response.json()) as ImageClassificationResponse;\n        }\n        default:\n          throw new OpenModelsError('Unsupported task');\n      }\n    } catch (error) {\n      if (error instanceof Error) throw new OpenModelsError(error.message);\n      throw new OpenModelsError('Unknown error occurred');\n    }\n  }\n}\n\nexport function client(config?: OpenModelsConfig): OpenModels {\n  return new OpenModels(config);\n}\n\n","import * as fs from 'fs';\nimport * as path from 'path';\nimport * as os from 'os';\n\nexport function readConfig(): any {\n  const configPath = path.join(os.homedir(), '.openmodels', 'config.json');\n  \n  if (!fs.existsSync(configPath)) {\n    return {\n      baseUrl: 'https://modal.run/api/v1'\n    };\n  }\n  \n  try {\n    const config = JSON.parse(fs.readFileSync(configPath, 'utf8'));\n    \n    // Handle both 'base-url' and 'baseUrl' formats\n    if (config['base-url'] && !config.baseUrl) {\n      config.baseUrl = config['base-url'];\n    }\n    \n    return config;\n  } catch (error) {\n    console.log('Warning: Could not read config file, using defaults');\n    return {\n      baseUrl: 'https://modal.run/api/v1'\n    };\n  }\n}\n","import { Command } from 'commander';\r\nimport { client } from '../../src';\r\nimport chalk from 'chalk';\r\nimport ora from 'ora';\r\nimport { readConfig } from '../utils';\r\n\r\nexport const embedCommand = new Command('embed')\r\n  .description('Generate text embeddings')\r\n  .argument('<text>', 'Text to embed')\r\n  .option('-m, --model <model>', 'Embedding model to use', 'sentence-transformers/all-MiniLM-L6-v2')\r\n  .option('-f, --format <format>', 'Output format (json, values)', 'json')\r\n  .option('-u, --url <url>', 'Custom embedding backend URL')\r\n  .action(async (text, options) => {\r\n    const config = readConfig();\r\n    \r\n    // Use custom URL if provided, otherwise try to get embedding URL from config\r\n    const baseUrl = options.url || config.embedUrl || config.baseUrl;\r\n    \r\n    const openmodels = client({ baseUrl });\r\n\r\n    const spinner = ora('Generating embedding...').start();\r\n    \r\n    try {\r\n      const response = await openmodels.embed({\r\n        model: options.model,\r\n        input: text,\r\n      });\r\n\r\n      spinner.stop();\r\n\r\n      if (options.format === 'values') {\r\n        console.log(chalk.blue('Embedding values:'));\r\n        console.log(response.data[0].embedding.slice(0, 10).map((v: number) => v.toFixed(4)).join(', '));\r\n        console.log(chalk.gray(`... (${response.data[0].embedding.length} dimensions)`));\r\n      } else {\r\n        console.log(chalk.blue('Embedding response:'));\r\n        console.log(JSON.stringify(response, null, 2));\r\n      }\r\n    } catch (error) {\r\n      spinner.stop();\r\n      console.error(chalk.red('Error:'), error);\r\n      process.exit(1);\r\n    }\r\n  });\r\n","import { Command } from 'commander';\r\nimport { client } from '../../src';\r\nimport chalk from 'chalk';\r\nimport ora from 'ora';\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport { readConfig } from '../utils';\r\n\r\nexport const imageCommand = new Command('image')\r\n  .description('Generate images from text prompts')\r\n  .argument('<prompt>', 'Text prompt for image generation')\r\n  .option('-m, --model <model>', 'Image model to use', 'stabilityai/stable-diffusion-xl-base-1.0')\r\n  .option('-s, --size <size>', 'Image size', '1024x1024')\r\n  .option('-q, --quality <quality>', 'Image quality (standard, hd)', 'standard')\r\n  .option('-n, --number <number>', 'Number of images to generate', '1')\r\n  .option('-o, --output <file>', 'Output file path')\r\n  .option('-u, --url <url>', 'Custom image backend URL')\r\n  .action(async (prompt, options) => {\r\n    const config = readConfig();\r\n    \r\n    // Use custom URL if provided, otherwise try to get image URL from config\r\n    const baseUrl = options.url || config.imageUrl || config.baseUrl;\r\n    \r\n    const openmodels = client({ baseUrl });\r\n\r\n    const spinner = ora('Generating image...').start();\r\n    \r\n    try {\r\n      const response = await openmodels.image({\r\n        model: options.model,\r\n        prompt: prompt,\r\n        size: options.size,\r\n        quality: options.quality,\r\n        n: parseInt(options.number)\r\n      });\r\n\r\n      spinner.stop();\r\n\r\n      const imageData = response.data[0];\r\n      if (imageData.b64_json) {\r\n        const imageBuffer = Buffer.from(imageData.b64_json, 'base64');\r\n        \r\n        const filename = options.output || `generated_image_${Date.now()}.png`;\r\n        const filepath = path.resolve(filename);\r\n        \r\n        fs.writeFileSync(filepath, imageBuffer);\r\n        \r\n        console.log(chalk.green('✓ Image generated successfully!'));\r\n        console.log(chalk.blue('Saved to:'), filepath);\r\n        console.log(chalk.gray(`Model: ${response.model}`));\r\n        console.log(chalk.gray(`Size: ${options.size}`));\r\n      }\r\n    } catch (error) {\r\n      spinner.stop();\r\n      console.error(chalk.red('Error:'), error);\r\n      process.exit(1);\r\n    }\r\n  });\r\n","import { Command } from 'commander';\r\nimport chalk from 'chalk';\r\nimport { readConfig } from '../utils';\r\n\r\nexport const modelsCommand = new Command('models')\r\n  .description('List available models')\r\n  .option('-t, --type <type>', 'Filter by model type (text, embed, image)', 'all')\r\n  .action(async (options) => {\r\n    const config = readConfig();\r\n    \r\n    const models = {\r\n      text: [\r\n        'microsoft/DialoGPT-medium',\r\n        'microsoft/DialoGPT-large',\r\n        'facebook/blenderbot-400M-distill',\r\n        'EleutherAI/gpt-neo-2.7B',\r\n        'EleutherAI/gpt-j-6B',\r\n        'microsoft/DialoGPT-small',\r\n        'distilgpt2'\r\n      ],\r\n      embed: [\r\n        'sentence-transformers/all-MiniLM-L6-v2',\r\n        'sentence-transformers/all-mpnet-base-v2',\r\n        'BAAI/bge-large-en',\r\n        'BAAI/bge-base-en',\r\n        'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\r\n        'sentence-transformers/all-MiniLM-L12-v2'\r\n      ],\r\n      image: [\r\n        'stabilityai/stable-diffusion-xl-base-1.0',\r\n        'runwayml/stable-diffusion-v1-5',\r\n        'stabilityai/stable-diffusion-2-1',\r\n        'CompVis/stable-diffusion-v1-4'\r\n      ]\r\n    };\r\n\r\n    if (options.type === 'all') {\r\n      console.log(chalk.blue('Available Models:'));\r\n      console.log();\r\n      \r\n      console.log(chalk.green('Text Generation Models:'));\r\n      models.text.forEach(model => {\r\n        console.log(`  ${chalk.cyan('•')} ${model}`);\r\n      });\r\n      \r\n      console.log();\r\n      console.log(chalk.green('Embedding Models:'));\r\n      models.embed.forEach(model => {\r\n        console.log(`  ${chalk.cyan('•')} ${model}`);\r\n      });\r\n      \r\n      console.log();\r\n      console.log(chalk.green('Image Generation Models:'));\r\n      models.image.forEach(model => {\r\n        console.log(`  ${chalk.cyan('•')} ${model}`);\r\n      });\r\n    } else if (models[options.type as keyof typeof models]) {\r\n      console.log(chalk.blue(`${options.type.charAt(0).toUpperCase() + options.type.slice(1)} Models:`));\r\n      models[options.type as keyof typeof models].forEach(model => {\r\n        console.log(`  ${chalk.cyan('•')} ${model}`);\r\n      });\r\n    } else {\r\n      console.log(chalk.red('Error: Invalid model type. Use: text, embed, image, or all'));\r\n      process.exit(1);\r\n    }\r\n  });\r\n","import { Command } from 'commander';\r\nimport chalk from 'chalk';\r\nimport * as fs from 'fs';\r\nimport * as path from 'path';\r\nimport * as os from 'os';\r\n\r\nexport const configCommand = new Command('config')\r\n  .description('Manage OpenModels configuration')\r\n  .command('set')\r\n  .description('Set configuration values')\r\n  .argument('<key>', 'Configuration key (api-key, base-url, embed-url, image-url)')\r\n  .argument('<value>', 'Configuration value')\r\n  .action(async (key, value) => {\r\n    const configPath = path.join(os.homedir(), '.openmodels', 'config.json');\r\n    const configDir = path.dirname(configPath);\r\n    \r\n    // Ensure config directory exists\r\n    if (!fs.existsSync(configDir)) {\r\n      fs.mkdirSync(configDir, { recursive: true });\r\n    }\r\n    \r\n    // Read existing config\r\n    let config: any = {};\r\n    if (fs.existsSync(configPath)) {\r\n      try {\r\n        config = JSON.parse(fs.readFileSync(configPath, 'utf8'));\r\n      } catch (error) {\r\n        console.log(chalk.yellow('Warning: Could not read existing config, creating new one'));\r\n      }\r\n    }\r\n    \r\n    // Update config\r\n    config[key] = value;\r\n    \r\n    // Write config\r\n    fs.writeFileSync(configPath, JSON.stringify(config, null, 2));\r\n    \r\n    console.log(chalk.green(`✓ Set ${key} = ${value}`));\r\n  });\r\n\r\nconfigCommand\r\n  .command('get')\r\n  .description('Get configuration values')\r\n  .argument('[key]', 'Configuration key to get (optional)')\r\n  .action(async (key) => {\r\n    const config = readConfig();\r\n    \r\n    if (key) {\r\n      if (config[key]) {\r\n        console.log(chalk.blue(`${key}:`), config[key]);\r\n      } else {\r\n        console.log(chalk.red(`Configuration key '${key}' not found`));\r\n        process.exit(1);\r\n      }\r\n    } else {\r\n      console.log(chalk.blue('Current configuration:'));\r\n      Object.entries(config).forEach(([k, v]) => {\r\n        console.log(`  ${chalk.cyan(k)}: ${v}`);\r\n      });\r\n    }\r\n  });\r\n\r\nconfigCommand\r\n  .command('list')\r\n  .description('List all configuration values')\r\n  .action(async () => {\r\n    const config = readConfig();\r\n    \r\n    console.log(chalk.blue('Current configuration:'));\r\n    Object.entries(config).forEach(([k, v]) => {\r\n      console.log(`  ${chalk.cyan(k)}: ${v}`);\r\n    });\r\n  });\r\n\r\nexport function readConfig(): any {\r\n  const configPath = path.join(os.homedir(), '.openmodels', 'config.json');\r\n  \r\n  if (!fs.existsSync(configPath)) {\r\n    return {\r\n      baseUrl: 'https://modal.run/api/v1'\r\n    };\r\n  }\r\n  \r\n  try {\r\n    return JSON.parse(fs.readFileSync(configPath, 'utf8'));\r\n  } catch (error) {\r\n    console.log(chalk.yellow('Warning: Could not read config file, using defaults'));\r\n    return {\r\n      baseUrl: 'https://modal.run/api/v1'\r\n    };\r\n  }\r\n}\r\n"],"mappings":";8dAEA,IAAAA,EAAwB,qBCFxB,IAAAC,EAAwB,qBCAxB,IAAAC,EAAkB,yBCYX,IAAeC,EAAf,KAAgD,CACrD,YAAsBC,EAAwB,CAAxB,YAAAA,CAAyB,CAQjD,ECnBO,IAAMC,EAAN,cAA8B,KAAM,CACzC,YACEC,EACOC,EACAC,EACP,CACA,MAAMF,CAAO,EAHN,YAAAC,EACA,UAAAC,EAGP,KAAK,KAAO,iBACd,CACF,EAEA,eAAuBC,EACrBC,EACuC,CACvC,GAAI,CAACA,EAAS,KACZ,MAAM,IAAIL,EAAgB,uBAAuB,EAInD,GAAIK,EAAS,SAAW,IACtB,MAAM,IAAIL,EAAgB,kDAAmD,GAAG,EAElF,GAAIK,EAAS,SAAW,IACtB,MAAM,IAAIL,EAAgB,oDAAqD,GAAG,EAKpF,IAAMM,GADO,MAAMD,EAAS,KAAK,GACd,MAAM;AAAA,CAAI,EAE7B,QAAWE,KAAQD,EAAO,CACxB,IAAME,EAAUD,EAAK,KAAK,EAE1B,GAAIC,IAAY,GAChB,IAAIA,IAAY,SAAU,OAE1B,GAAIA,EAAQ,WAAW,QAAQ,EAAG,CAChC,IAAMC,EAAOD,EAAQ,MAAM,CAAC,EAC5B,GAAIC,IAAS,SAAU,OAEvB,GAAI,CACF,IAAMC,EAAsB,KAAK,MAAMD,CAAI,EACvCC,EAAO,UAAU,CAAC,GAAG,OAAO,UAC9B,MAAMA,EAAO,QAAQ,CAAC,EAAE,MAAM,QAElC,MAAY,CAEV,QACF,CACF,EACF,CACF,CFhDO,IAAMC,EAAN,cAA4BC,CAAa,CAC9C,MAAM,KAAKC,EAA8C,CACvD,IAAMC,EAAM,GAAG,KAAK,OAAO,OAAO,QAE5BC,EAAkC,CACtC,eAAgB,mBAChB,cAAiB,UAAU,KAAK,OAAO,MAAM,EAC/C,EAEMC,EAAW,QAAM,EAAAC,SAAMH,EAAK,CAChC,OAAQ,OACR,QAAAC,EACA,KAAM,KAAK,UAAUF,CAAO,CAC9B,CAAC,EAED,GAAI,CAACG,EAAS,GAAI,CAChB,IAAME,EAAY,MAAMF,EAAS,KAAK,EAEtC,MAAIA,EAAS,SAAW,IAChB,IAAIG,EAAgB,kDAAmD,GAAG,EAE9EH,EAAS,SAAW,IAChB,IAAIG,EAAgB,oDAAqD,GAAG,EAG9E,IAAIA,EAAgB,oBAAoBH,EAAS,MAAM,IAAIE,CAAS,GAAIF,EAAS,MAAM,CAC/F,CAEA,OAAOA,CACT,CAEA,MAAM,MAAMH,EAAyC,CACnD,IAAMC,EAAM,GAAG,KAAK,OAAO,OAAO,SAE5BC,EAAkC,CACtC,eAAgB,mBAChB,cAAiB,UAAU,KAAK,OAAO,MAAM,EAC/C,EAEMC,EAAW,QAAM,EAAAC,SAAMH,EAAK,CAChC,OAAQ,OACR,QAAAC,EACA,KAAM,KAAK,UAAUF,CAAO,CAC9B,CAAC,EAED,GAAI,CAACG,EAAS,GAAI,CAChB,IAAME,EAAY,MAAMF,EAAS,KAAK,EAEtC,MAAIA,EAAS,SAAW,IAChB,IAAIG,EAAgB,kDAAmD,GAAG,EAE9EH,EAAS,SAAW,IAChB,IAAIG,EAAgB,oDAAqD,GAAG,EAG9E,IAAIA,EAAgB,oBAAoBH,EAAS,MAAM,IAAIE,CAAS,GAAIF,EAAS,MAAM,CAC/F,CAEA,OAAOA,CACT,CAEA,MAAM,MAAMH,EAAqC,CAC/C,IAAMC,EAAM,GAAG,KAAK,OAAO,OAAO,SAE5BC,EAAkC,CACtC,eAAgB,mBAChB,cAAiB,UAAU,KAAK,OAAO,MAAM,EAC/C,EAEMC,EAAW,QAAM,EAAAC,SAAMH,EAAK,CAChC,OAAQ,OACR,QAAAC,EACA,KAAM,KAAK,UAAUF,CAAO,CAC9B,CAAC,EAED,GAAI,CAACG,EAAS,GAAI,CAChB,IAAME,EAAY,MAAMF,EAAS,KAAK,EAEtC,MAAIA,EAAS,SAAW,IAChB,IAAIG,EAAgB,kDAAmD,GAAG,EAE9EH,EAAS,SAAW,IAChB,IAAIG,EAAgB,oDAAqD,GAAG,EAG9E,IAAIA,EAAgB,oBAAoBH,EAAS,MAAM,IAAIE,CAAS,GAAIF,EAAS,MAAM,CAC/F,CAEA,OAAOA,CACT,CAEA,MAAM,gBAAgBH,EAA+C,CACnE,IAAMC,EAAM,GAAG,KAAK,OAAO,OAAO,cAC5BC,EAAkC,CACtC,eAAgB,mBAChB,cAAiB,UAAU,KAAK,OAAO,MAAM,EAC/C,EACMC,EAAW,QAAM,EAAAC,SAAMH,EAAK,CAAE,OAAQ,OAAQ,QAAAC,EAAS,KAAM,KAAK,UAAUF,CAAO,CAAE,CAAC,EAC5F,GAAI,CAACG,EAAS,GAAI,CAChB,IAAME,EAAY,MAAMF,EAAS,KAAK,EACtC,MAAIA,EAAS,SAAW,IAAW,IAAIG,EAAgB,kDAAmD,GAAG,EACzGH,EAAS,SAAW,IAAW,IAAIG,EAAgB,oDAAqD,GAAG,EACzG,IAAIA,EAAgB,oBAAoBH,EAAS,MAAM,IAAIE,CAAS,GAAIF,EAAS,MAAM,CAC/F,CACA,OAAOA,CACT,CAEA,MAAM,eAAeH,EAA8C,CACjE,IAAMC,EAAM,GAAG,KAAK,OAAO,OAAO,aAC5BC,EAAkC,CACtC,eAAgB,mBAChB,cAAiB,UAAU,KAAK,OAAO,MAAM,EAC/C,EACMC,EAAW,QAAM,EAAAC,SAAMH,EAAK,CAAE,OAAQ,OAAQ,QAAAC,EAAS,KAAM,KAAK,UAAUF,CAAO,CAAE,CAAC,EAC5F,GAAI,CAACG,EAAS,GAAI,CAChB,IAAME,EAAY,MAAMF,EAAS,KAAK,EACtC,MAAIA,EAAS,SAAW,IAAW,IAAIG,EAAgB,kDAAmD,GAAG,EACzGH,EAAS,SAAW,IAAW,IAAIG,EAAgB,oDAAqD,GAAG,EACzG,IAAIA,EAAgB,oBAAoBH,EAAS,MAAM,IAAIE,CAAS,GAAIF,EAAS,MAAM,CAC/F,CACA,OAAOA,CACT,CAEA,MAAM,eAAeH,EAAmD,CACtE,IAAMC,EAAM,GAAG,KAAK,OAAO,OAAO,YAC5BC,EAAkC,CACtC,eAAgB,mBAChB,cAAiB,UAAU,KAAK,OAAO,MAAM,EAC/C,EACMC,EAAW,QAAM,EAAAC,SAAMH,EAAK,CAAE,OAAQ,OAAQ,QAAAC,EAAS,KAAM,KAAK,UAAUF,CAAO,CAAE,CAAC,EAC5F,GAAI,CAACG,EAAS,GAAI,CAChB,IAAME,EAAY,MAAMF,EAAS,KAAK,EACtC,MAAIA,EAAS,SAAW,IAAW,IAAIG,EAAgB,kDAAmD,GAAG,EACzGH,EAAS,SAAW,IAAW,IAAIG,EAAgB,oDAAqD,GAAG,EACzG,IAAIA,EAAgB,oBAAoBH,EAAS,MAAM,IAAIE,CAAS,GAAIF,EAAS,MAAM,CAC/F,CACA,OAAOA,CACT,CACF,EG7IO,IAAMI,GAAyC,CACpD,uBAAwB,CACtB,8BACA,6BACA,8BACF,EACA,kBAAmB,CACjB,4BACA,kCACF,EACA,UAAa,CACX,wCACF,EACA,mBAAoB,CAClB,qBACF,EACA,kBAAmB,CACjB,yBACF,EACA,mBAAoB,CAClB,gCACF,CACF,EAEO,SAASC,EAAgBC,EAAoB,CAClD,IAAMC,EAASH,GAAeE,CAAI,EAClC,GAAI,CAACC,GAAUA,EAAO,SAAW,EAC/B,MAAM,IAAI,MAAM,0CAA0CD,CAAI,EAAE,EAElE,OAAOC,EAAO,CAAC,CACjB,CCXO,IAAMC,EAAN,KAAiB,CAOtB,YAAYC,EAA2B,CAAC,EAAG,CAEzC,GAAI,CAACA,EAAO,OACV,MAAM,IAAIC,EAAgB,6EAA6E,EAIzG,GAAI,CAACD,EAAO,OAAO,WAAW,KAAK,GAAKA,EAAO,OAAO,OAAS,GAC7D,MAAM,IAAIC,EAAgB,4FAA4F,EAIxH,IAAMC,EAAUF,EAAO,SAAW,uBAI5BG,EAAiBC,GACjBF,EAAQ,SAAS,WAAW,GAAKA,EAAQ,SAAS,OAAO,EAEpDA,EACEA,EAAQ,SAAS,GAAG,EAEtB,GAAGA,CAAO,QAAQE,CAAO,GAGzB,4BAA4BA,CAAO,GAI9C,KAAK,aAAe,IAAIC,EAAc,CACpC,OAAQL,EAAO,OACf,QAASG,EAAc,MAAM,CAC/B,CAAC,EAED,KAAK,cAAgB,IAAIE,EAAc,CACrC,OAAQL,EAAO,OACf,QAASG,EAAc,OAAO,CAChC,CAAC,EAED,KAAK,cAAgB,IAAIE,EAAc,CACrC,OAAQL,EAAO,OACf,QAASG,EAAc,OAAO,CAChC,CAAC,EAED,KAAK,cAAgB,IAAIE,EAAc,CACrC,OAAQL,EAAO,OACf,QAASG,EAAc,OAAO,CAChC,CAAC,EAED,KAAK,eAAiB,IAAIE,EAAc,CACtC,OAAQL,EAAO,OACf,QAASG,EAAc,QAAQ,CACjC,CAAC,CACH,CAEA,MAAM,KAAKG,EAAyG,CAClH,GAAI,CACF,IAAMC,EAAW,MAAM,KAAK,aAAa,KAAKD,CAAO,EAErD,OAAIA,EAAQ,OACHE,EAAeD,CAAQ,EAGnB,MAAMA,EAAS,KAAK,CAEnC,OAASE,EAAO,CACd,MAAIA,aAAiB,MACb,IAAIR,EAAgBQ,EAAM,OAAO,EAEnC,IAAIR,EAAgB,wBAAwB,CACpD,CACF,CAEA,MAAM,MAAMK,EAAuD,CACjE,GAAI,CAGF,OADa,MADI,MAAM,KAAK,cAAc,MAAMA,CAAO,GAC3B,KAAK,CAEnC,OAASG,EAAO,CACd,MAAIA,aAAiB,MACb,IAAIR,EAAgBQ,EAAM,OAAO,EAEnC,IAAIR,EAAgB,wBAAwB,CACpD,CACF,CAEA,MAAM,MAAMK,EAA+C,CACzD,GAAI,CAGF,OADa,MADI,MAAM,KAAK,cAAc,MAAMA,CAAO,GAC3B,KAAK,CAEnC,OAASG,EAAO,CACd,MAAIA,aAAiB,MACb,IAAIR,EAAgBQ,EAAM,OAAO,EAEnC,IAAIR,EAAgB,wBAAwB,CACpD,CACF,CAEA,MAAM,IAAIK,EAAmF,CAC3F,GAAI,CACF,OAAQA,EAAQ,KAAM,CACpB,IAAK,kBAAmB,CACtB,IAAMI,EAASJ,EAAgB,OAASK,EAAgB,iBAAiB,EACnEC,EAAiC,CAAE,GAAGN,EAAS,MAAAI,CAAM,EACrDH,EAAW,MAAM,KAAK,aAAa,KAAKK,CAAO,EACrD,OAAIA,EAAQ,OAAeJ,EAAeD,CAAQ,EAC1C,MAAMA,EAAS,KAAK,CAC9B,CACA,IAAK,mBAAoB,CACvB,IAAMG,EAASJ,EAAgB,OAASK,EAAgB,kBAAkB,EACpEE,EAAuB,CAAE,GAAGP,EAAS,MAAAI,CAAM,EAEjD,OAAQ,MADS,MAAM,KAAK,cAAc,MAAMG,CAAM,GAC/B,KAAK,CAC9B,CACA,IAAK,YAAa,CAChB,IAAMH,EAASJ,EAAgB,OAASK,EAAgB,WAAW,EAC7DG,EAA2B,CAAE,GAAGR,EAAS,MAAAI,CAAM,EAErD,OAAQ,MADS,MAAM,KAAK,cAAc,MAAMI,CAAM,GAC/B,KAAK,CAC9B,CACA,IAAK,mBAAoB,CACvB,IAAMJ,EAASJ,EAAgB,OAASK,EAAgB,kBAAkB,EACpEI,EAA+B,CAAE,GAAGT,EAAS,MAAAI,CAAM,EAEzD,OAAQ,MADS,MAAM,KAAK,cAAc,gBAAiBK,CAAI,GACxC,KAAK,CAC9B,CACA,IAAK,kBAAmB,CACtB,IAAML,EAASJ,EAAgB,OAASK,EAAgB,iBAAiB,EACnEI,EAA8B,CAAE,GAAGT,EAAS,MAAAI,CAAM,EAExD,OAAQ,MADS,MAAM,KAAK,cAAc,eAAgBK,CAAI,GACvC,KAAK,CAC9B,CACA,IAAK,uBAAwB,CAC3B,IAAML,EAASJ,EAAgB,OAASK,EAAgB,sBAAsB,EACxEK,EAAmC,CAAE,GAAGV,EAAS,MAAAI,CAAM,EAE7D,OAAQ,MADS,MAAM,KAAK,eAAe,eAAgBM,CAAI,GACxC,KAAK,CAC9B,CACA,QACE,MAAM,IAAIf,EAAgB,kBAAkB,CAChD,CACF,OAASQ,EAAO,CACd,MAAIA,aAAiB,MAAa,IAAIR,EAAgBQ,EAAM,OAAO,EAC7D,IAAIR,EAAgB,wBAAwB,CACpD,CACF,CACF,EAEO,SAASgB,EAAOjB,EAAuC,CAC5D,OAAO,IAAID,EAAWC,CAAM,CAC9B,CLjLA,IAAAkB,EAAkB,oBAClBC,EAAgB,kBMHhB,IAAAC,EAAoB,iBACpBC,EAAsB,mBACtBC,EAAoB,iBAEb,SAASC,GAAkB,CAChC,IAAMC,EAAkB,OAAQ,UAAQ,EAAG,cAAe,aAAa,EAEvE,GAAI,CAAI,aAAWA,CAAU,EAC3B,MAAO,CACL,QAAS,0BACX,EAGF,GAAI,CACF,IAAMC,EAAS,KAAK,MAAS,eAAaD,EAAY,MAAM,CAAC,EAG7D,OAAIC,EAAO,UAAU,GAAK,CAACA,EAAO,UAChCA,EAAO,QAAUA,EAAO,UAAU,GAG7BA,CACT,MAAgB,CACd,eAAQ,IAAI,qDAAqD,EAC1D,CACL,QAAS,0BACX,CACF,CACF,CNtBO,IAAMC,EAAc,IAAI,UAAQ,MAAM,EAC1C,YAAY,qBAAqB,EACjC,SAAS,YAAa,2BAA2B,EACjD,OAAO,sBAAuB,eAAgB,2BAA2B,EACzE,OAAO,eAAgB,sBAAuB,EAAK,EACnD,OAAO,2BAA4B,6BAA8B,KAAK,EACtE,OAAO,4BAA6B,6BAA8B,KAAK,EACvE,OAAO,oBAAqB,wBAAyB,EAAK,EAC1D,OAAO,MAAOC,EAASC,IAAY,CAClC,IAAMC,EAASC,EAAW,EACpBC,EAAaC,EAAOH,CAAM,EAE5BD,EAAQ,YACV,MAAMK,GAAgBF,EAAYH,CAAO,EAChCD,EACT,MAAMO,GAAWH,EAAYJ,EAASC,CAAO,GAE7C,QAAQ,IAAI,EAAAO,QAAM,IAAI,2DAA2D,CAAC,EAClF,QAAQ,KAAK,CAAC,EAElB,CAAC,EAEH,eAAeD,GAAWH,EAAiBJ,EAAiBC,EAAc,CACxE,IAAMQ,KAAU,EAAAC,SAAI,wBAAwB,EAAE,MAAM,EAEpD,GAAI,CACF,IAAMC,EAAU,CACd,MAAOV,EAAQ,MACf,SAAU,CACR,CAAE,KAAM,OAAQ,QAASD,CAAQ,CACnC,EACA,WAAY,SAASC,EAAQ,SAAS,EACtC,YAAa,WAAWA,EAAQ,WAAW,EAC3C,OAAQA,EAAQ,MAClB,EAEA,GAAIA,EAAQ,OAAQ,CAClBQ,EAAQ,KAAK,EACb,IAAMG,EAAS,MAAMR,EAAW,KAAKO,CAAO,EAE5C,QAAQ,OAAO,MAAM,EAAAH,QAAM,KAAK,YAAY,CAAC,EAC7C,cAAiBK,KAASD,EACxB,QAAQ,OAAO,MAAMC,CAAK,EAE5B,QAAQ,IAAI,CACd,KAAO,CACL,IAAMC,EAAW,MAAMV,EAAW,KAAKO,CAAO,EAC9CF,EAAQ,KAAK,EAEb,QAAQ,IAAI,EAAAD,QAAM,KAAK,WAAW,CAAC,EACnC,QAAQ,IAAIM,EAAS,QAAQ,CAAC,EAAE,QAAQ,OAAO,CACjD,CACF,OAASC,EAAO,CACdN,EAAQ,KAAK,EACb,QAAQ,MAAM,EAAAD,QAAM,IAAI,QAAQ,EAAGO,CAAK,EACxC,QAAQ,KAAK,CAAC,CAChB,CACF,CAEA,eAAeT,GAAgBF,EAAiBH,EAAc,CAC5D,QAAQ,IAAI,EAAAO,QAAM,MAAM,6CAA6C,CAAC,EACtE,QAAQ,IAAI,EAAAA,QAAM,KAAK,gBAAgBP,EAAQ,KAAK,EAAE,CAAC,EAGvD,IAAMe,EADW,QAAQ,UAAU,EACf,gBAAgB,CAClC,MAAO,QAAQ,MACf,OAAQ,QAAQ,MAClB,CAAC,EAEKC,EAAc,IAAM,CACxBD,EAAG,SAAS,EAAAR,QAAM,KAAK,OAAO,EAAG,MAAOU,GAAkB,CACxD,GAAIA,EAAM,YAAY,IAAM,OAAQ,CAClCF,EAAG,MAAM,EACT,MACF,CAEA,IAAMP,KAAU,EAAAC,SAAI,wBAAwB,EAAE,MAAM,EAEpD,GAAI,CACF,IAAMC,EAAU,CACd,MAAOV,EAAQ,MACf,SAAU,CACR,CAAE,KAAM,OAAQ,QAASiB,CAAM,CACjC,EACA,WAAY,SAASjB,EAAQ,SAAS,EACtC,YAAa,WAAWA,EAAQ,WAAW,EAC3C,OAAQ,EACV,EAEMa,EAAW,MAAMV,EAAW,KAAKO,CAAO,EAC9CF,EAAQ,KAAK,EAEb,QAAQ,IAAI,EAAAD,QAAM,MAAM,KAAK,EAAGM,EAAS,QAAQ,CAAC,EAAE,QAAQ,OAAO,EACnE,QAAQ,IAAI,EACZG,EAAY,CACd,OAASF,EAAO,CACdN,EAAQ,KAAK,EACb,QAAQ,MAAM,EAAAD,QAAM,IAAI,QAAQ,EAAGO,CAAK,EACxCE,EAAY,CACd,CACF,CAAC,CACH,EAEAA,EAAY,CACd,CO9GA,IAAAE,EAAwB,qBAExB,IAAAC,EAAkB,oBAClBC,EAAgB,kBAGT,IAAMC,EAAe,IAAI,UAAQ,OAAO,EAC5C,YAAY,0BAA0B,EACtC,SAAS,SAAU,eAAe,EAClC,OAAO,sBAAuB,yBAA0B,wCAAwC,EAChG,OAAO,wBAAyB,+BAAgC,MAAM,EACtE,OAAO,kBAAmB,8BAA8B,EACxD,OAAO,MAAOC,EAAMC,IAAY,CAC/B,IAAMC,EAASC,EAAW,EAGpBC,EAAUH,EAAQ,KAAOC,EAAO,UAAYA,EAAO,QAEnDG,EAAaC,EAAO,CAAE,QAAAF,CAAQ,CAAC,EAE/BG,KAAU,EAAAC,SAAI,yBAAyB,EAAE,MAAM,EAErD,GAAI,CACF,IAAMC,EAAW,MAAMJ,EAAW,MAAM,CACtC,MAAOJ,EAAQ,MACf,MAAOD,CACT,CAAC,EAEDO,EAAQ,KAAK,EAETN,EAAQ,SAAW,UACrB,QAAQ,IAAI,EAAAS,QAAM,KAAK,mBAAmB,CAAC,EAC3C,QAAQ,IAAID,EAAS,KAAK,CAAC,EAAE,UAAU,MAAM,EAAG,EAAE,EAAE,IAAKE,GAAcA,EAAE,QAAQ,CAAC,CAAC,EAAE,KAAK,IAAI,CAAC,EAC/F,QAAQ,IAAI,EAAAD,QAAM,KAAK,QAAQD,EAAS,KAAK,CAAC,EAAE,UAAU,MAAM,cAAc,CAAC,IAE/E,QAAQ,IAAI,EAAAC,QAAM,KAAK,qBAAqB,CAAC,EAC7C,QAAQ,IAAI,KAAK,UAAUD,EAAU,KAAM,CAAC,CAAC,EAEjD,OAASG,EAAO,CACdL,EAAQ,KAAK,EACb,QAAQ,MAAM,EAAAG,QAAM,IAAI,QAAQ,EAAGE,CAAK,EACxC,QAAQ,KAAK,CAAC,CAChB,CACF,CAAC,EC3CH,IAAAC,EAAwB,qBAExB,IAAAC,EAAkB,oBAClBC,EAAgB,kBAChBC,EAAoB,iBACpBC,EAAsB,mBAGf,IAAMC,EAAe,IAAI,UAAQ,OAAO,EAC5C,YAAY,mCAAmC,EAC/C,SAAS,WAAY,kCAAkC,EACvD,OAAO,sBAAuB,qBAAsB,0CAA0C,EAC9F,OAAO,oBAAqB,aAAc,WAAW,EACrD,OAAO,0BAA2B,+BAAgC,UAAU,EAC5E,OAAO,wBAAyB,+BAAgC,GAAG,EACnE,OAAO,sBAAuB,kBAAkB,EAChD,OAAO,kBAAmB,0BAA0B,EACpD,OAAO,MAAOC,EAAQC,IAAY,CACjC,IAAMC,EAASC,EAAW,EAGpBC,EAAUH,EAAQ,KAAOC,EAAO,UAAYA,EAAO,QAEnDG,EAAaC,EAAO,CAAE,QAAAF,CAAQ,CAAC,EAE/BG,KAAU,EAAAC,SAAI,qBAAqB,EAAE,MAAM,EAEjD,GAAI,CACF,IAAMC,EAAW,MAAMJ,EAAW,MAAM,CACtC,MAAOJ,EAAQ,MACf,OAAQD,EACR,KAAMC,EAAQ,KACd,QAASA,EAAQ,QACjB,EAAG,SAASA,EAAQ,MAAM,CAC5B,CAAC,EAEDM,EAAQ,KAAK,EAEb,IAAMG,EAAYD,EAAS,KAAK,CAAC,EACjC,GAAIC,EAAU,SAAU,CACtB,IAAMC,EAAc,OAAO,KAAKD,EAAU,SAAU,QAAQ,EAEtDE,EAAWX,EAAQ,QAAU,mBAAmB,KAAK,IAAI,CAAC,OAC1DY,EAAgB,UAAQD,CAAQ,EAEnC,gBAAcC,EAAUF,CAAW,EAEtC,QAAQ,IAAI,EAAAG,QAAM,MAAM,sCAAiC,CAAC,EAC1D,QAAQ,IAAI,EAAAA,QAAM,KAAK,WAAW,EAAGD,CAAQ,EAC7C,QAAQ,IAAI,EAAAC,QAAM,KAAK,UAAUL,EAAS,KAAK,EAAE,CAAC,EAClD,QAAQ,IAAI,EAAAK,QAAM,KAAK,SAASb,EAAQ,IAAI,EAAE,CAAC,CACjD,CACF,OAASc,EAAO,CACdR,EAAQ,KAAK,EACb,QAAQ,MAAM,EAAAO,QAAM,IAAI,QAAQ,EAAGC,CAAK,EACxC,QAAQ,KAAK,CAAC,CAChB,CACF,CAAC,ECzDH,IAAAC,EAAwB,qBACxBC,EAAkB,oBAGX,IAAMC,EAAgB,IAAI,UAAQ,QAAQ,EAC9C,YAAY,uBAAuB,EACnC,OAAO,oBAAqB,4CAA6C,KAAK,EAC9E,OAAO,MAAOC,GAAY,CACzB,IAAMC,EAASC,EAAW,EAEpBC,EAAS,CACb,KAAM,CACJ,4BACA,2BACA,mCACA,0BACA,sBACA,2BACA,YACF,EACA,MAAO,CACL,yCACA,0CACA,oBACA,mBACA,8DACA,yCACF,EACA,MAAO,CACL,2CACA,iCACA,mCACA,+BACF,CACF,EAEIH,EAAQ,OAAS,OACnB,QAAQ,IAAI,EAAAI,QAAM,KAAK,mBAAmB,CAAC,EAC3C,QAAQ,IAAI,EAEZ,QAAQ,IAAI,EAAAA,QAAM,MAAM,yBAAyB,CAAC,EAClDD,EAAO,KAAK,QAAQE,GAAS,CAC3B,QAAQ,IAAI,KAAK,EAAAD,QAAM,KAAK,QAAG,CAAC,IAAIC,CAAK,EAAE,CAC7C,CAAC,EAED,QAAQ,IAAI,EACZ,QAAQ,IAAI,EAAAD,QAAM,MAAM,mBAAmB,CAAC,EAC5CD,EAAO,MAAM,QAAQE,GAAS,CAC5B,QAAQ,IAAI,KAAK,EAAAD,QAAM,KAAK,QAAG,CAAC,IAAIC,CAAK,EAAE,CAC7C,CAAC,EAED,QAAQ,IAAI,EACZ,QAAQ,IAAI,EAAAD,QAAM,MAAM,0BAA0B,CAAC,EACnDD,EAAO,MAAM,QAAQE,GAAS,CAC5B,QAAQ,IAAI,KAAK,EAAAD,QAAM,KAAK,QAAG,CAAC,IAAIC,CAAK,EAAE,CAC7C,CAAC,GACQF,EAAOH,EAAQ,IAA2B,GACnD,QAAQ,IAAI,EAAAI,QAAM,KAAK,GAAGJ,EAAQ,KAAK,OAAO,CAAC,EAAE,YAAY,EAAIA,EAAQ,KAAK,MAAM,CAAC,CAAC,UAAU,CAAC,EACjGG,EAAOH,EAAQ,IAA2B,EAAE,QAAQK,GAAS,CAC3D,QAAQ,IAAI,KAAK,EAAAD,QAAM,KAAK,QAAG,CAAC,IAAIC,CAAK,EAAE,CAC7C,CAAC,IAED,QAAQ,IAAI,EAAAD,QAAM,IAAI,4DAA4D,CAAC,EACnF,QAAQ,KAAK,CAAC,EAElB,CAAC,ECjEH,IAAAE,EAAwB,qBACxBC,EAAkB,oBAClBC,EAAoB,iBACpBC,EAAsB,mBACtBC,EAAoB,iBAEPC,EAAgB,IAAI,UAAQ,QAAQ,EAC9C,YAAY,iCAAiC,EAC7C,QAAQ,KAAK,EACb,YAAY,0BAA0B,EACtC,SAAS,QAAS,6DAA6D,EAC/E,SAAS,UAAW,qBAAqB,EACzC,OAAO,MAAOC,EAAKC,IAAU,CAC5B,IAAMC,EAAkB,OAAQ,UAAQ,EAAG,cAAe,aAAa,EACjEC,EAAiB,UAAQD,CAAU,EAGjC,aAAWC,CAAS,GACvB,YAAUA,EAAW,CAAE,UAAW,EAAK,CAAC,EAI7C,IAAIC,EAAc,CAAC,EACnB,GAAO,aAAWF,CAAU,EAC1B,GAAI,CACFE,EAAS,KAAK,MAAS,eAAaF,EAAY,MAAM,CAAC,CACzD,MAAgB,CACd,QAAQ,IAAI,EAAAG,QAAM,OAAO,2DAA2D,CAAC,CACvF,CAIFD,EAAOJ,CAAG,EAAIC,EAGX,gBAAcC,EAAY,KAAK,UAAUE,EAAQ,KAAM,CAAC,CAAC,EAE5D,QAAQ,IAAI,EAAAC,QAAM,MAAM,cAASL,CAAG,MAAMC,CAAK,EAAE,CAAC,CACpD,CAAC,EAEHF,EACG,QAAQ,KAAK,EACb,YAAY,0BAA0B,EACtC,SAAS,QAAS,qCAAqC,EACvD,OAAO,MAAOC,GAAQ,CACrB,IAAMI,EAASE,EAAW,EAEtBN,EACEI,EAAOJ,CAAG,EACZ,QAAQ,IAAI,EAAAK,QAAM,KAAK,GAAGL,CAAG,GAAG,EAAGI,EAAOJ,CAAG,CAAC,GAE9C,QAAQ,IAAI,EAAAK,QAAM,IAAI,sBAAsBL,CAAG,aAAa,CAAC,EAC7D,QAAQ,KAAK,CAAC,IAGhB,QAAQ,IAAI,EAAAK,QAAM,KAAK,wBAAwB,CAAC,EAChD,OAAO,QAAQD,CAAM,EAAE,QAAQ,CAAC,CAACG,EAAGC,CAAC,IAAM,CACzC,QAAQ,IAAI,KAAK,EAAAH,QAAM,KAAKE,CAAC,CAAC,KAAKC,CAAC,EAAE,CACxC,CAAC,EAEL,CAAC,EAEHT,EACG,QAAQ,MAAM,EACd,YAAY,+BAA+B,EAC3C,OAAO,SAAY,CAClB,IAAMK,EAASE,EAAW,EAE1B,QAAQ,IAAI,EAAAD,QAAM,KAAK,wBAAwB,CAAC,EAChD,OAAO,QAAQD,CAAM,EAAE,QAAQ,CAAC,CAACG,EAAGC,CAAC,IAAM,CACzC,QAAQ,IAAI,KAAK,EAAAH,QAAM,KAAKE,CAAC,CAAC,KAAKC,CAAC,EAAE,CACxC,CAAC,CACH,CAAC,EAEI,SAASF,GAAkB,CAChC,IAAMJ,EAAkB,OAAQ,UAAQ,EAAG,cAAe,aAAa,EAEvE,GAAI,CAAI,aAAWA,CAAU,EAC3B,MAAO,CACL,QAAS,0BACX,EAGF,GAAI,CACF,OAAO,KAAK,MAAS,eAAaA,EAAY,MAAM,CAAC,CACvD,MAAgB,CACd,eAAQ,IAAI,EAAAG,QAAM,OAAO,qDAAqD,CAAC,EACxE,CACL,QAAS,0BACX,CACF,CACF,CXlFA,IAAMI,EAAU,IAAI,UAEpBA,EACG,KAAK,YAAY,EACjB,YAAY,gDAAgD,EAC5D,QAAQ,OAAO,EAGlBA,EAAQ,WAAWC,CAAW,EAC9BD,EAAQ,WAAWE,CAAY,EAC/BF,EAAQ,WAAWG,CAAY,EAC/BH,EAAQ,WAAWI,CAAa,EAChCJ,EAAQ,WAAWK,CAAa,EAEhCL,EAAQ,MAAM","names":["import_commander","import_commander","import_node_fetch","BaseProvider","config","OpenModelsError","message","status","code","parseSSEStream","response","lines","line","trimmed","data","parsed","ModalProvider","BaseProvider","request","url","headers","response","fetch","errorText","OpenModelsError","TASK_TO_MODELS","getDefaultModel","task","models","OpenModels","config","OpenModelsError","baseUrl","getServiceUrl","service","ModalProvider","request","response","parseSSEStream","error","model","getDefaultModel","chatReq","imgReq","embReq","aReq","vReq","client","import_chalk","import_ora","fs","path","os","readConfig","configPath","config","chatCommand","message","options","config","readConfig","openmodels","client","interactiveChat","singleChat","chalk","spinner","ora","request","stream","token","response","error","rl","askQuestion","input","import_commander","import_chalk","import_ora","embedCommand","text","options","config","readConfig","baseUrl","openmodels","client","spinner","ora","response","chalk","v","error","import_commander","import_chalk","import_ora","fs","path","imageCommand","prompt","options","config","readConfig","baseUrl","openmodels","client","spinner","ora","response","imageData","imageBuffer","filename","filepath","chalk","error","import_commander","import_chalk","modelsCommand","options","config","readConfig","models","chalk","model","import_commander","import_chalk","fs","path","os","configCommand","key","value","configPath","configDir","config","chalk","readConfig","k","v","program","chatCommand","embedCommand","imageCommand","modelsCommand","configCommand"]}