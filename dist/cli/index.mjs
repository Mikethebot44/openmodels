#!/usr/bin/env node
import{a as O,f as p}from"../chunk-7S7FWHQT.mjs";import{Command as J}from"commander";import{Command as q}from"commander";import m from"chalk";import I from"ora";import*as y from"fs";import*as k from"path";import*as $ from"os";function g(){let o=k.join($.homedir(),".openmodels","config.json");if(!y.existsSync(o))return{baseUrl:"https://modal.run/api/v1"};try{let e=JSON.parse(y.readFileSync(o,"utf8"));return e["base-url"]&&!e.baseUrl&&(e.baseUrl=e["base-url"]),e}catch{return console.log("Warning: Could not read config file, using defaults"),{baseUrl:"https://modal.run/api/v1"}}}var M=new q("chat").description("Chat with AI models").argument("[message]","Message to send to the AI").option("-m, --model <model>","Model to use","microsoft/DialoGPT-medium").option("-s, --stream","Stream the response",!1).option("-t, --temperature <temp>","Temperature for generation","0.7").option("-k, --max-tokens <tokens>","Maximum tokens to generate","200").option("-i, --interactive","Interactive chat mode",!1).action(async(o,e)=>{let t=g(),n=p(t);e.interactive?await F(n,e):o?await T(n,o,e):(console.log(m.red("Error: Please provide a message or use --interactive mode")),process.exit(1))});async function T(o,e,t){let n=I("Generating response...").start();try{let r={model:t.model,messages:[{role:"user",content:e}],max_tokens:parseInt(t.maxTokens),temperature:parseFloat(t.temperature),stream:t.stream};if(t.stream){n.stop();let a=await o.chat(r);process.stdout.write(m.blue("Response: "));for await(let s of a)process.stdout.write(s);console.log()}else{let a=await o.chat(r);n.stop(),console.log(m.blue("Response:")),console.log(a.choices[0].message.content)}}catch(r){n.stop(),console.error(m.red("Error:"),r),process.exit(1)}}async function F(o,e){console.log(m.green('Interactive chat mode. Type "exit" to quit.')),console.log(m.gray(`Using model: ${e.model}`));let n=O("readline").createInterface({input:process.stdin,output:process.stdout}),r=()=>{n.question(m.blue("You: "),async a=>{if(a.toLowerCase()==="exit"){n.close();return}let s=I("Generating response...").start();try{let d={model:e.model,messages:[{role:"user",content:a}],max_tokens:parseInt(e.maxTokens),temperature:parseFloat(e.temperature),stream:!1},x=await o.chat(d);s.stop(),console.log(m.green("AI:"),x.choices[0].message.content),console.log(),r()}catch(d){s.stop(),console.error(m.red("Error:"),d),r()}})};r()}import{Command as D}from"commander";import h from"chalk";import P from"ora";var S=new D("embed").description("Generate text embeddings").argument("<text>","Text to embed").option("-m, --model <model>","Embedding model to use","sentence-transformers/all-MiniLM-L6-v2").option("-f, --format <format>","Output format (json, values)","json").option("-u, --url <url>","Custom embedding backend URL").action(async(o,e)=>{let t=g(),n=e.url||t.embedUrl||t.baseUrl,r=p({baseUrl:n}),a=P("Generating embedding...").start();try{let s=await r.embed({model:e.model,input:o});a.stop(),e.format==="values"?(console.log(h.blue("Embedding values:")),console.log(s.data[0].embedding.slice(0,10).map(d=>d.toFixed(4)).join(", ")),console.log(h.gray(`... (${s.data[0].embedding.length} dimensions)`))):(console.log(h.blue("Embedding response:")),console.log(JSON.stringify(s,null,2)))}catch(s){a.stop(),console.error(h.red("Error:"),s),process.exit(1)}});import{Command as z}from"commander";import u from"chalk";import B from"ora";import*as E from"fs";import*as U from"path";var j=new z("image").description("Generate images from text prompts").argument("<prompt>","Text prompt for image generation").option("-m, --model <model>","Image model to use","stabilityai/stable-diffusion-xl-base-1.0").option("-s, --size <size>","Image size","1024x1024").option("-q, --quality <quality>","Image quality (standard, hd)","standard").option("-n, --number <number>","Number of images to generate","1").option("-o, --output <file>","Output file path").option("-u, --url <url>","Custom image backend URL").action(async(o,e)=>{let t=g(),n=e.url||t.imageUrl||t.baseUrl,r=p({baseUrl:n}),a=B("Generating image...").start();try{let s=await r.image({model:e.model,prompt:o,size:e.size,quality:e.quality,n:parseInt(e.number)});a.stop();let d=s.data[0];if(d.b64_json){let x=Buffer.from(d.b64_json,"base64"),A=e.output||`generated_image_${Date.now()}.png`,w=U.resolve(A);E.writeFileSync(w,x),console.log(u.green("\u2713 Image generated successfully!")),console.log(u.blue("Saved to:"),w),console.log(u.gray(`Model: ${s.model}`)),console.log(u.gray(`Size: ${e.size}`))}}catch(s){a.stop(),console.error(u.red("Error:"),s),process.exit(1)}});import{Command as N}from"commander";import l from"chalk";var G=new N("models").description("List available models").option("-t, --type <type>","Filter by model type (text, embed, image)","all").action(async o=>{let e=g(),t={text:["microsoft/DialoGPT-medium","microsoft/DialoGPT-large","facebook/blenderbot-400M-distill","EleutherAI/gpt-neo-2.7B","EleutherAI/gpt-j-6B","microsoft/DialoGPT-small","distilgpt2"],embed:["sentence-transformers/all-MiniLM-L6-v2","sentence-transformers/all-mpnet-base-v2","BAAI/bge-large-en","BAAI/bge-base-en","sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2","sentence-transformers/all-MiniLM-L12-v2"],image:["stabilityai/stable-diffusion-xl-base-1.0","runwayml/stable-diffusion-v1-5","stabilityai/stable-diffusion-2-1","CompVis/stable-diffusion-v1-4"]};o.type==="all"?(console.log(l.blue("Available Models:")),console.log(),console.log(l.green("Text Generation Models:")),t.text.forEach(n=>{console.log(`  ${l.cyan("\u2022")} ${n}`)}),console.log(),console.log(l.green("Embedding Models:")),t.embed.forEach(n=>{console.log(`  ${l.cyan("\u2022")} ${n}`)}),console.log(),console.log(l.green("Image Generation Models:")),t.image.forEach(n=>{console.log(`  ${l.cyan("\u2022")} ${n}`)})):t[o.type]?(console.log(l.blue(`${o.type.charAt(0).toUpperCase()+o.type.slice(1)} Models:`)),t[o.type].forEach(n=>{console.log(`  ${l.cyan("\u2022")} ${n}`)})):(console.log(l.red("Error: Invalid model type. Use: text, embed, image, or all")),process.exit(1))});import{Command as _}from"commander";import c from"chalk";import*as i from"fs";import*as b from"path";import*as v from"os";var C=new _("config").description("Manage OpenModels configuration").command("set").description("Set configuration values").argument("<key>","Configuration key (api-key, base-url, embed-url, image-url)").argument("<value>","Configuration value").action(async(o,e)=>{let t=b.join(v.homedir(),".openmodels","config.json"),n=b.dirname(t);i.existsSync(n)||i.mkdirSync(n,{recursive:!0});let r={};if(i.existsSync(t))try{r=JSON.parse(i.readFileSync(t,"utf8"))}catch{console.log(c.yellow("Warning: Could not read existing config, creating new one"))}r[o]=e,i.writeFileSync(t,JSON.stringify(r,null,2)),console.log(c.green(`\u2713 Set ${o} = ${e}`))});C.command("get").description("Get configuration values").argument("[key]","Configuration key to get (optional)").action(async o=>{let e=L();o?e[o]?console.log(c.blue(`${o}:`),e[o]):(console.log(c.red(`Configuration key '${o}' not found`)),process.exit(1)):(console.log(c.blue("Current configuration:")),Object.entries(e).forEach(([t,n])=>{console.log(`  ${c.cyan(t)}: ${n}`)}))});C.command("list").description("List all configuration values").action(async()=>{let o=L();console.log(c.blue("Current configuration:")),Object.entries(o).forEach(([e,t])=>{console.log(`  ${c.cyan(e)}: ${t}`)})});function L(){let o=b.join(v.homedir(),".openmodels","config.json");if(!i.existsSync(o))return{baseUrl:"https://modal.run/api/v1"};try{return JSON.parse(i.readFileSync(o,"utf8"))}catch{return console.log(c.yellow("Warning: Could not read config file, using defaults")),{baseUrl:"https://modal.run/api/v1"}}}var f=new J;f.name("openmodels").description("CLI for OpenModels - Open-source AI models SDK").version("0.3.0");f.addCommand(M);f.addCommand(S);f.addCommand(j);f.addCommand(G);f.addCommand(C);f.parse();
//# sourceMappingURL=index.mjs.map